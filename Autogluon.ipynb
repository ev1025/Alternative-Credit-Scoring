{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b918b07d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T06:01:51.444811Z",
     "iopub.status.busy": "2025-01-06T06:01:51.444553Z",
     "iopub.status.idle": "2025-01-06T06:02:16.972243Z",
     "shell.execute_reply": "2025-01-06T06:02:16.971155Z"
    },
    "papermill": {
     "duration": 25.534039,
     "end_time": "2025-01-06T06:02:16.973913",
     "exception": false,
     "start_time": "2025-01-06T06:01:51.439874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray==2.10.0\r\n",
      "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (8.1.7)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (3.16.1)\r\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (4.23.0)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (1.0.8)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (24.1)\r\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (3.20.3)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (1.4.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (2.32.3)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.10.0) (24.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.10.0) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.10.0) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.10.0) (0.20.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.10.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.10.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.10.0) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.10.0) (2024.8.30)\r\n",
      "Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ray\r\n",
      "  Attempting uninstall: ray\r\n",
      "    Found existing installation: ray 2.40.0\r\n",
      "    Uninstalling ray-2.40.0:\r\n",
      "      Successfully uninstalled ray-2.40.0\r\n",
      "Successfully installed ray-2.10.0\r\n",
      "Collecting autogluon.tabular\r\n",
      "  Downloading autogluon.tabular-1.2-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: numpy<2.1.4,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (1.26.4)\r\n",
      "Requirement already satisfied: scipy<1.16,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (1.13.1)\r\n",
      "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (2.1.4)\r\n",
      "Collecting scikit-learn<1.5.3,>=1.4.0 (from autogluon.tabular)\r\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (3.3)\r\n",
      "Collecting autogluon.core==1.2 (from autogluon.tabular)\r\n",
      "  Downloading autogluon.core-1.2-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting autogluon.features==1.2 (from autogluon.tabular)\r\n",
      "  Downloading autogluon.features-1.2-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.2->autogluon.tabular) (4.66.5)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.2->autogluon.tabular) (2.32.3)\r\n",
      "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.2->autogluon.tabular) (3.7.1)\r\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.2->autogluon.tabular) (1.35.83)\r\n",
      "Collecting autogluon.common==1.2 (from autogluon.core==1.2->autogluon.tabular)\r\n",
      "  Downloading autogluon.common-1.2-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: psutil<7.0.0,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.2->autogluon.core==1.2->autogluon.tabular) (5.9.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.3,>=1.4.0->autogluon.tabular) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.3,>=1.4.0->autogluon.tabular) (3.5.0)\r\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.83 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.tabular) (1.35.83)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.tabular) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.tabular) (0.10.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular) (4.53.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular) (24.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular) (3.1.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.2->autogluon.tabular) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.2->autogluon.tabular) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.2->autogluon.tabular) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.2->autogluon.tabular) (2024.8.30)\r\n",
      "Downloading autogluon.tabular-1.2-py3-none-any.whl (352 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.core-1.2-py3-none-any.whl (266 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/266.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.features-1.2-py3-none-any.whl (64 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.common-1.2-py3-none-any.whl (68 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "Successfully installed autogluon.common-1.2 autogluon.core-1.2 autogluon.features-1.2 autogluon.tabular-1.2 scikit-learn-1.5.2\r\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (8.1.5)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (4.0.13)\r\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (71.0.4)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ray==2.10.0\n",
    "!pip install autogluon.tabular\n",
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15be05e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-06T06:02:16.984129Z",
     "iopub.status.busy": "2025-01-06T06:02:16.983829Z",
     "iopub.status.idle": "2025-01-06T06:02:17.584885Z",
     "shell.execute_reply": "2025-01-06T06:02:17.584000Z"
    },
    "papermill": {
     "duration": 0.607325,
     "end_time": "2025-01-06T06:02:17.586274",
     "exception": false,
     "start_time": "2025-01-06T06:02:16.978949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/alter-data/credit(120).csv\n",
      "/kaggle/input/alter-data/alter_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba301cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T06:02:17.595719Z",
     "iopub.status.busy": "2025-01-06T06:02:17.595412Z",
     "iopub.status.idle": "2025-01-06T06:02:19.438686Z",
     "shell.execute_reply": "2025-01-06T06:02:19.437916Z"
    },
    "papermill": {
     "duration": 1.849408,
     "end_time": "2025-01-06T06:02:19.440133",
     "exception": false,
     "start_time": "2025-01-06T06:02:17.590725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소비 빈도 및 활동성</th>\n",
       "      <th>소비 금액 및 규모</th>\n",
       "      <th>연체 가능성 및 신용 리스크</th>\n",
       "      <th>서비스 이용 다양성</th>\n",
       "      <th>남성</th>\n",
       "      <th>여성</th>\n",
       "      <th>20대</th>\n",
       "      <th>30대</th>\n",
       "      <th>40대</th>\n",
       "      <th>50대</th>\n",
       "      <th>60대</th>\n",
       "      <th>70대</th>\n",
       "      <th>연체여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.641394</td>\n",
       "      <td>0.768399</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.409465</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586476</td>\n",
       "      <td>0.652896</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.454870</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.460871</td>\n",
       "      <td>0.612550</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.438317</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.497903</td>\n",
       "      <td>0.563611</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.426953</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.515722</td>\n",
       "      <td>0.523879</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.420061</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   소비 빈도 및 활동성  소비 금액 및 규모  연체 가능성 및 신용 리스크  서비스 이용 다양성    남성     여성    20대  \\\n",
       "0     0.641394    0.768399         0.383721    0.409465  True  False   True   \n",
       "1     0.586476    0.652896         0.372093    0.454870  True  False  False   \n",
       "2     0.460871    0.612550         0.348837    0.438317  True  False  False   \n",
       "3     0.497903    0.563611         0.279070    0.426953  True  False  False   \n",
       "4     0.515722    0.523879         0.267442    0.420061  True  False  False   \n",
       "\n",
       "     30대    40대    50대    60대    70대  연체여부  \n",
       "0  False  False  False  False  False   1.0  \n",
       "1   True  False  False  False  False   1.0  \n",
       "2   True  False  False  False  False   1.0  \n",
       "3  False   True  False  False  False   0.0  \n",
       "4  False   True  False  False  False   0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소비 빈도 및 활동성</th>\n",
       "      <th>소비 금액 및 규모</th>\n",
       "      <th>연체 가능성 및 신용 리스크</th>\n",
       "      <th>서비스 이용 다양성</th>\n",
       "      <th>남성</th>\n",
       "      <th>여성</th>\n",
       "      <th>20대</th>\n",
       "      <th>30대</th>\n",
       "      <th>40대</th>\n",
       "      <th>50대</th>\n",
       "      <th>60대</th>\n",
       "      <th>70대</th>\n",
       "      <th>회원여부_연체</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.384426</td>\n",
       "      <td>0.077502</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.265004</td>\n",
       "      <td>0.042410</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.076045</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119652</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168577</td>\n",
       "      <td>0.040089</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   소비 빈도 및 활동성  소비 금액 및 규모  연체 가능성 및 신용 리스크  서비스 이용 다양성  남성  여성  20대  30대  \\\n",
       "0     0.384426    0.077502              0.5        0.25   0   1    0    0   \n",
       "1     0.265004    0.042410              1.0        0.25   0   1    1    0   \n",
       "2     0.076045    0.038398              0.5        0.25   0   1    0    0   \n",
       "3     0.119652    0.045946              0.5        0.25   0   1    0    1   \n",
       "4     0.168577    0.040089              0.5        0.25   1   0    0    0   \n",
       "\n",
       "   40대  50대  60대  70대  회원여부_연체  \n",
       "0    1    0    0    0        1  \n",
       "1    0    0    0    0        1  \n",
       "2    0    1    0    0        1  \n",
       "3    0    0    0    0        1  \n",
       "4    0    0    1    0        1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_df_train = pd.read_csv('/kaggle/input/alter-data/alter_data.csv')\n",
    "auto_df_test = pd.read_csv('/kaggle/input/alter-data/credit(120).csv')\n",
    "\n",
    "display(auto_df_train.head())\n",
    "display(auto_df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee08add",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T06:02:19.450232Z",
     "iopub.status.busy": "2025-01-06T06:02:19.449973Z",
     "iopub.status.idle": "2025-01-06T06:02:19.467838Z",
     "shell.execute_reply": "2025-01-06T06:02:19.467132Z"
    },
    "papermill": {
     "duration": 0.024411,
     "end_time": "2025-01-06T06:02:19.469275",
     "exception": false,
     "start_time": "2025-01-06T06:02:19.444864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소비 빈도 및 활동성</th>\n",
       "      <th>소비 금액 및 규모</th>\n",
       "      <th>연체 가능성 및 신용 리스크</th>\n",
       "      <th>서비스 이용 다양성</th>\n",
       "      <th>남성</th>\n",
       "      <th>여성</th>\n",
       "      <th>20대</th>\n",
       "      <th>30대</th>\n",
       "      <th>40대</th>\n",
       "      <th>50대</th>\n",
       "      <th>60대</th>\n",
       "      <th>70대</th>\n",
       "      <th>연체여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.641394</td>\n",
       "      <td>0.768399</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.409465</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586476</td>\n",
       "      <td>0.652896</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.454870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.460871</td>\n",
       "      <td>0.612550</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.438317</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.497903</td>\n",
       "      <td>0.563611</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.426953</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.515722</td>\n",
       "      <td>0.523879</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.420061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   소비 빈도 및 활동성  소비 금액 및 규모  연체 가능성 및 신용 리스크  서비스 이용 다양성  남성  여성  20대  30대  \\\n",
       "0     0.641394    0.768399         0.383721    0.409465   1   0    1    0   \n",
       "1     0.586476    0.652896         0.372093    0.454870   1   0    0    1   \n",
       "2     0.460871    0.612550         0.348837    0.438317   1   0    0    1   \n",
       "3     0.497903    0.563611         0.279070    0.426953   1   0    0    0   \n",
       "4     0.515722    0.523879         0.267442    0.420061   1   0    0    0   \n",
       "\n",
       "   40대  50대  60대  70대  연체여부  \n",
       "0    0    0    0    0   1.0  \n",
       "1    0    0    0    0   1.0  \n",
       "2    0    0    0    0   1.0  \n",
       "3    1    0    0    0   0.0  \n",
       "4    1    0    0    0   0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_df_train.iloc[:, 4:]= auto_df_train.iloc[:, 4:].astype(int)\n",
    "auto_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49b0562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T06:02:19.479652Z",
     "iopub.status.busy": "2025-01-06T06:02:19.479433Z",
     "iopub.status.idle": "2025-01-06T06:02:19.492620Z",
     "shell.execute_reply": "2025-01-06T06:02:19.491818Z"
    },
    "papermill": {
     "duration": 0.019614,
     "end_time": "2025-01-06T06:02:19.493827",
     "exception": false,
     "start_time": "2025-01-06T06:02:19.474213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소비 빈도 및 활동성</th>\n",
       "      <th>소비 금액 및 규모</th>\n",
       "      <th>연체 가능성 및 신용 리스크</th>\n",
       "      <th>서비스 이용 다양성</th>\n",
       "      <th>남성</th>\n",
       "      <th>여성</th>\n",
       "      <th>20대</th>\n",
       "      <th>30대</th>\n",
       "      <th>40대</th>\n",
       "      <th>50대</th>\n",
       "      <th>60대</th>\n",
       "      <th>70대</th>\n",
       "      <th>연체여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.384426</td>\n",
       "      <td>0.077502</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.265004</td>\n",
       "      <td>0.042410</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.076045</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119652</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168577</td>\n",
       "      <td>0.040089</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   소비 빈도 및 활동성  소비 금액 및 규모  연체 가능성 및 신용 리스크  서비스 이용 다양성  남성  여성  20대  30대  \\\n",
       "0     0.384426    0.077502              0.5        0.25   0   1    0    0   \n",
       "1     0.265004    0.042410              1.0        0.25   0   1    1    0   \n",
       "2     0.076045    0.038398              0.5        0.25   0   1    0    0   \n",
       "3     0.119652    0.045946              0.5        0.25   0   1    0    1   \n",
       "4     0.168577    0.040089              0.5        0.25   1   0    0    0   \n",
       "\n",
       "   40대  50대  60대  70대  연체여부  \n",
       "0    1    0    0    0     1  \n",
       "1    0    0    0    0     1  \n",
       "2    0    1    0    0     1  \n",
       "3    0    0    0    0     1  \n",
       "4    0    0    1    0     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_df_test.rename(columns={'회원여부_연체': '연체여부'}, inplace=True)\n",
    "auto_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5be9e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T06:02:19.504568Z",
     "iopub.status.busy": "2025-01-06T06:02:19.504358Z",
     "iopub.status.idle": "2025-01-06T14:02:21.653324Z",
     "shell.execute_reply": "2025-01-06T14:02:21.652108Z"
    },
    "papermill": {
     "duration": 28802.15615,
     "end_time": "2025-01-06T14:02:21.654818",
     "exception": false,
     "start_time": "2025-01-06T06:02:19.498668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250106_060220\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       30.08 GB / 31.35 GB (96.0%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 7200s of the 28800s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-01-06 06:02:23,854\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20250106_060220/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Beginning AutoGluon training ... Time limit = 7196s\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250106_060220/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Train Data Rows:    30950\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Train Data Columns: 12\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Label Column:       연체여부\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tAvailable Memory:                    30338.84 MB\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tTrain Data (Original)  Memory Usage: 2.83 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\t('float', []) : 4 | ['소비 빈도 및 활동성', '소비 금액 및 규모', '연체 가능성 및 신용 리스크', '서비스 이용 다양성']\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\t('int', [])   : 8 | ['남성', '여성', '20대', '30대', '40대', ...]\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\t('float', [])     : 4 | ['소비 빈도 및 활동성', '소비 금액 및 규모', '연체 가능성 및 신용 리스크', '서비스 이용 다양성']\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t\t('int', ['bool']) : 8 | ['남성', '여성', '20대', '30대', '40대', ...]\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.1s = Fit runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t12 features in original data used to generate 12 features in processed data.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tTrain Data (Processed) Memory Usage: 1.18 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 4795.87s of the 7195.60s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.4328\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 4795.09s of the 7194.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.4247\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4794.89s of the 7194.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=352)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=352)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=351)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=352)\u001b[0m 1 warning generated.\u001b[32m [repeated 60x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=420)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=351)\u001b[0m 1 warning generated.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=516)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=586)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5386\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t27.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.54s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 4762.73s of the 7162.45s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=631)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=632)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=698)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=700)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=841)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5378\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t18.92s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.32s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 4742.02s of the 7141.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5119\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t6.15s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 4734.19s of the 7133.91s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=843)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.515\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t7.25s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 4725.30s of the 7125.02s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=941)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=941)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1019)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1019)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1105)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1105)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1191)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1191)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1234)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1234)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.534\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t91.73s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 4631.93s of the 7031.65s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5256\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t3.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.26s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 4626.50s of the 7026.22s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.52\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t3.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.26s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 4621.13s of the 7020.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=1313)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=1383)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1457)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1530)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5382\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t133.15s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.43s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 4486.27s of the 6885.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=1556)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=1604)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:38] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1604)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1604)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=1604)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1604)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=1604)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=1604)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=1603)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:40] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1603)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1603)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1603)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1603)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=1662)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:42] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1660)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1660)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1660)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1660)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1662)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:44] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1662)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1662)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1662)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1662)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:49] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5394\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t13.27s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 4471.01s of the 6870.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1776)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5422\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t177.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 4291.65s of the 6691.38s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:49] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:07:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=1778)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=2108)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2177)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2276)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2351)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5361\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t24.91s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.23s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 4264.94s of the 6664.66s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=2389)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2555)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2555)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2641)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2641)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2684)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2684)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5351\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t75.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 4187.69s of the 6587.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5452\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t254.8s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 3931.00s of the 6330.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=3007)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3076)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3179)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3249)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5369\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t27.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.66s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 3901.27s of the 6300.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=3287)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=3358)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3358)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_ray_fit pid=3360)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=3431)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=3467)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=3503)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=3539)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5347\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t178.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.4s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 3721.29s of the 6121.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.09%)\n",
      "\u001b[36m(_ray_fit pid=3576)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3576)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3655)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3655)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3743)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3743)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3829)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3829)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5347\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t28.88s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 3690.44s of the 6090.17s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=3862)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3862)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=3922)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3923)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.482116\tvalid_set's mcc: 0.531678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=3991)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4026)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4052)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4026)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.477215\tvalid_set's mcc: 0.55054\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4096)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4026)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.469911\tvalid_set's mcc: 0.554875\n",
      "\u001b[36m(_ray_fit pid=4096)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.479399\tvalid_set's mcc: 0.541559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=4131)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5368\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t46.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t3.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 3642.21s of the 6041.93s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\u001b[36m(_ray_fit pid=4158)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5468\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t199.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 3441.11s of the 5840.83s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.16%)\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:03] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=4476)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:07] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m Potential solutions:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4534)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:17] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m \u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4647)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4590)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4590)\u001b[0m Potential solutions:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4590)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4590)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4590)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:20] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m \u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:25:23] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4678)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5368\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t19.63s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 3419.76s of the 5819.48s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5158\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t4.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 3413.59s of the 5813.31s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=4723)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4723)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4802)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4802)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4888)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4888)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4974)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4974)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.534\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t98.29s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 3313.65s of the 5713.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=5016)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5016)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5068)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=5140)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5211)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5284)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5372\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t35.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 3276.41s of the 5676.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.10%)\n",
      "\u001b[36m(_ray_fit pid=5358)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5358)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5286)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=5438)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5438)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5481)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5481)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5524)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5524)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5567)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5567)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5610)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5610)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5653)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5653)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5352\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t401.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 2872.56s of the 5272.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5111\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t13.21s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 2857.84s of the 5257.56s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=5718)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5786)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5856)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5924)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5960)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5405\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t33.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 2822.46s of the 5222.18s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=5997)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=6070)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6142)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6214)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5378\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t269.41s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.76s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 2550.98s of the 4950.71s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=6241)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:39:54] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:39:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:39:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=6287)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=6346)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:39:57] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6346)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6346)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:39:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6346)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6346)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6344)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:40:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6344)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6344)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6344)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6344)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:40:04] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:40:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5366\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t14.91s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 2534.15s of the 4933.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:40:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6460)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5433\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t312.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 2219.66s of the 4619.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:40:05] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:40:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:40:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=6487)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=6792)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6861)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6931)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7001)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5388\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t23.41s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 2193.99s of the 4593.72s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5465\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t180.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 2011.59s of the 4411.31s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=7029)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7347)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7347)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=7424)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7424)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7512)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7512)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7598)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7598)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5348\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t22.82s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 1986.89s of the 4386.62s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=7692)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=7623)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7623)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=7691)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=7691)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=7762)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=7798)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=7834)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=7870)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=7906)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=7942)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.534\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t239.79s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.74s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 1745.13s of the 4144.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.07%)\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:19] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7980)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=8038)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:23] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8038)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8038)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8038)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8038)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8036)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:25] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8036)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8036)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8036)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8036)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:30] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:32] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8152)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5386\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t13.83s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 1729.39s of the 4129.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5365\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t3.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.93s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 1724.55s of the 4124.27s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:31] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:53:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=8179)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=8225)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8225)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=8304)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8304)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8390)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8390)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8432)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8432)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=8476)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8476)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=8519)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8519)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5344\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t91.52s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 1631.39s of the 4031.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=8569)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=8570)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=8570)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=8640)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=8676)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=8676)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=8713)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=8740)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=8740)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=8785)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=8821)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5327\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t163.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 1465.75s of the 3865.48s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5425\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t82.77s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 1381.20s of the 3780.92s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.09%)\n",
      "\u001b[36m(_ray_fit pid=9131)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9199)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9268)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9332)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9374)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5337\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t46.15s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.8s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 1332.24s of the 3731.96s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=9410)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=9482)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9554)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9628)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5319\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t61.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 1268.73s of the 3668.45s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=9630)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=9703)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9703)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=9781)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9781)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9869)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9869)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9953)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9953)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5361\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t25.72s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 1240.93s of the 3640.65s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=9989)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=9989)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=10050)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=10120)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10193)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10266)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5365\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t41.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 1197.46s of the 3597.19s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.07%)\n",
      "\u001b[36m(_ray_fit pid=10268)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=10341)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10341)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.508539\tvalid_set's mcc: 0.519191\n",
      "\u001b[36m(_ray_fit pid=10340)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.508637\tvalid_set's mcc: 0.508263\n",
      "\u001b[36m(_ray_fit pid=10341)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.488338\tvalid_set's mcc: 0.525415\n",
      "\u001b[36m(_ray_fit pid=10340)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.491047\tvalid_set's mcc: 0.520728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10410)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10340)\u001b[0m [3000]\tvalid_set's binary_logloss: 0.484219\tvalid_set's mcc: 0.524939\n",
      "\u001b[36m(_ray_fit pid=10410)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.511297\tvalid_set's mcc: 0.49616\n",
      "\u001b[36m(_ray_fit pid=10410)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.491929\tvalid_set's mcc: 0.51731\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10340)\u001b[0m [6000]\tvalid_set's binary_logloss: 0.480662\tvalid_set's mcc: 0.523955\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10445)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=10480)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10445)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.507046\tvalid_set's mcc: 0.525463\n",
      "\u001b[36m(_ray_fit pid=10480)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.498701\tvalid_set's mcc: 0.538082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10515)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10445)\u001b[0m [3000]\tvalid_set's binary_logloss: 0.475106\tvalid_set's mcc: 0.54897\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10550)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10515)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.483445\tvalid_set's mcc: 0.54437\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10550)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.512378\tvalid_set's mcc: 0.508045\n",
      "\u001b[36m(_ray_fit pid=10515)\u001b[0m [3000]\tvalid_set's binary_logloss: 0.476062\tvalid_set's mcc: 0.547874\n",
      "\u001b[36m(_ray_fit pid=10515)\u001b[0m [4000]\tvalid_set's binary_logloss: 0.472771\tvalid_set's mcc: 0.549944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10585)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=10585)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.499724\tvalid_set's mcc: 0.530419\n",
      "\u001b[36m(_ray_fit pid=10585)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.477432\tvalid_set's mcc: 0.541401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5359\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t116.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t9.98s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 1077.02s of the 3476.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.516\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t13.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.97s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 1062.44s of the 3462.16s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=10636)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=10636)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=10715)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10715)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10757)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=10757)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=10801)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=10801)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=10887)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10887)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=10930)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=10930)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5366\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t93.33s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 967.45s of the 3367.18s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\u001b[36m(_ray_fit pid=10980)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=11051)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11125)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11198)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5367\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t276.34s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.72s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 688.98s of the 3088.70s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\u001b[36m(_ray_fit pid=11200)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5453\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t302.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 384.36s of the 2784.09s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.10%)\n",
      "\u001b[36m(_ray_fit pid=11547)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:00] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=11547)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=11547)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=11547)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=11547)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=11547)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=11547)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=11548)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=11548)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=11548)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=11548)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=11548)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:06] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m \u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11631)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11666)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:09] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m \u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=11664)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=11666)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=11666)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=11666)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=11666)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=11666)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:20] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:22] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m Potential solutions:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5306\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t22.36s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r15_BAG_L1 ... Training model for up to 360.06s of the 2759.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=11785)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:16:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11754)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=11853)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=11853)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.479557\tvalid_set's mcc: 0.524275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=11925)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=11853)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.480855\tvalid_set's mcc: 0.531987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=11959)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=11959)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.472793\tvalid_set's mcc: 0.550703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=11995)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5388\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t31.77s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.35s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 326.51s of the 2726.23s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=12029)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\u001b[36m(_ray_fit pid=12132)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 76)\n",
      "\u001b[36m(_ray_fit pid=12166)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 76)\n",
      "\u001b[36m(_ray_fit pid=12200)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 76)\n",
      "\u001b[36m(_ray_fit pid=12234)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 76)\n",
      "\u001b[36m(_ray_fit pid=12302)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 80)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5392\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t248.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r86_BAG_L1 ... Training model for up to 76.29s of the 2476.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.09%)\n",
      "\u001b[36m(_ray_fit pid=12338)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=12338)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=12416)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=12416)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=12459)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=12459)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=12501)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=12501)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=12545)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=12545)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=12553)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=12553)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=12632)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=12632)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5364\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t134.79s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 479.59s of the 2339.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_r22_BAG_L1': 0.4, 'NeuralNetFastAI_r191_BAG_L1': 0.08, 'CatBoost_r69_BAG_L1': 0.08, 'NeuralNetTorch_r41_BAG_L1': 0.08, 'XGBoost_BAG_L1': 0.04, 'CatBoost_r177_BAG_L1': 0.04, 'NeuralNetTorch_r79_BAG_L1': 0.04, 'CatBoost_r13_BAG_L1': 0.04, 'NeuralNetFastAI_r145_BAG_L1': 0.04, 'XGBoost_r89_BAG_L1': 0.04, 'NeuralNetTorch_r30_BAG_L1': 0.04, 'NeuralNetTorch_r86_BAG_L1': 0.04, 'CatBoost_r50_BAG_L1': 0.04}\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5478\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t4.73s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2334.52s of the 2334.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.20%)\n",
      "\u001b[36m(_ray_fit pid=12688)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=12783)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=12852)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=12897)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5532\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t20.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.25s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 2312.38s of the 2312.23s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.21%)\n",
      "\u001b[36m(_ray_fit pid=12973)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=12972)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=13067)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=13172)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5564\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t21.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 2288.39s of the 2288.24s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=13206)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5408\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t28.55s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 2258.21s of the 2258.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.544\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t36.47s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 2220.17s of the 2220.02s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.24%)\n",
      "\u001b[36m(_ray_fit pid=13284)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=13284)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=13362)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=13362)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=13405)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=13405)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=13448)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=13448)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=13534)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=13534)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5516\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t53.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 2165.22s of the 2165.07s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=13567)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=13567)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5413\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t4.39s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.31s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 2158.88s of the 2158.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5395\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t5.23s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.29s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 2151.73s of the 2151.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.28%)\n",
      "\u001b[36m(_ray_fit pid=13662)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=13661)\u001b[0m No improvement since epoch 1: early stopping\n",
      "\u001b[36m(_ray_fit pid=13661)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=13732)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=13768)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=13804)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=13840)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=13876)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=13912)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5433\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t132.81s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.47s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 2017.13s of the 2016.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.30%)\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:28:48] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:28:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:28:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=13956)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:28:52] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:28:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m \u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:28:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14039)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:28:59] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:28:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m \u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:29:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14127)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5565\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t13.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 2001.29s of the 2001.14s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.15%)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5495\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t136.81s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1862.68s of the 1862.53s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:29:00] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:29:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:29:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=14155)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\u001b[36m(_ray_fit pid=14471)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=14540)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14609)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14679)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.556\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t35.37s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 1825.24s of the 1825.10s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=14709)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.25%)\n",
      "\u001b[36m(_ray_fit pid=14759)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=14759)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=14837)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14837)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14922)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=14922)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15010)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15010)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5505\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t42.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 1780.83s of the 1780.68s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=15042)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=15042)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.15%)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5484\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t214.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 1564.41s of the 1564.26s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.24%)\n",
      "\u001b[36m(_ray_fit pid=15385)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=15452)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15523)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15594)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5551\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t25.79s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 1536.65s of the 1536.50s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=15628)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.28%)\n",
      "\u001b[36m(_ray_fit pid=15672)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=15671)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_ray_fit pid=15671)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=15743)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=15672)\u001b[0m No improvement since epoch 19: early stopping\n",
      "\u001b[36m(_ray_fit pid=15779)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=15815)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=15779)\u001b[0m No improvement since epoch 17: early stopping\n",
      "\u001b[36m(_ray_fit pid=15851)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=15887)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=15923)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5438\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t172.39s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 1362.32s of the 1362.17s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.50%)\n",
      "\u001b[36m(_ray_fit pid=15967)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=15967)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=16045)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16045)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16131)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16131)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16218)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16218)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5479\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t27.96s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 1332.33s of the 1332.18s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=16251)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=16251)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.19%)\n",
      "\u001b[36m(_ray_fit pid=16317)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=16386)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16481)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5468\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t17.94s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 1312.44s of the 1312.30s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=16525)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.16%)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5496\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t128.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 1182.41s of the 1182.26s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.88%)\n",
      "\u001b[36m(_ray_fit pid=16878)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:43] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=16878)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=16878)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=16878)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=16878)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=16878)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=16878)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=16879)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:46] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=16879)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=16879)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=16879)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=16879)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:49] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16939)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:52] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16939)\u001b[0m Potential solutions:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16939)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16939)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16939)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:55] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=16937)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:43:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=16997)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=17026)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:42:58] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=17026)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17026)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:43:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17026)\u001b[0m \u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17026)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:43:02] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:43:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m Potential solutions:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5569\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t24.89s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 1155.51s of the 1155.36s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:43:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17057)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.539\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t15.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 1138.21s of the 1138.07s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:43:03] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:43:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:43:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17084)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.19%)\n",
      "\u001b[36m(_ray_fit pid=17139)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=17139)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=17217)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17217)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17303)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17303)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17389)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17389)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5508\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t44.73s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=17431)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=17431)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 1091.67s of the 1091.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.28%)\n",
      "\u001b[36m(_ray_fit pid=17489)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=17488)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=17559)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17634)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17706)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5423\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t37.87s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 1051.89s of the 1051.74s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=17708)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.51%)\n",
      "\u001b[36m(_ray_fit pid=17787)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=17787)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=17868)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17868)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=17911)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=17911)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=17954)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=17954)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=18040)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18040)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18083)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=18083)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5505\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t238.33s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 811.41s of the 811.26s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5394\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t187.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t1.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 622.37s of the 622.23s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.32%)\n",
      "\u001b[36m(_ray_fit pid=18153)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=18222)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18292)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18363)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5515\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t26.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.27s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 593.50s of the 593.35s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=18390)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.28%)\n",
      "\u001b[36m(_ray_fit pid=18441)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=18440)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_ray_fit pid=18440)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=18511)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=18547)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=18583)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=18619)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=18655)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=18691)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5463\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t268.35s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.84s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 323.19s of the 323.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.30%)\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:02] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=18734)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=18849)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:09] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18849)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18849)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18849)\u001b[0m \u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18849)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18793)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18793)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18793)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18793)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18793)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5545\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t12.71s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r30_BAG_L2 ... Training model for up to 308.61s of the 308.46s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:12] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18909)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.15%)\n",
      "\u001b[36m(_ray_fit pid=19073)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 72)\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [07:57:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=18907)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5472\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t182.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: LightGBM_r130_BAG_L2 ... Training model for up to 124.76s of the 124.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.28%)\n",
      "\u001b[36m(_ray_fit pid=19252)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=19320)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19389)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19460)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5588\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t24.31s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: NeuralNetTorch_r86_BAG_L2 ... Training model for up to 98.58s of the 98.44s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.15%)\n",
      "\u001b[36m(_ray_fit pid=19604)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 29)\n",
      "\u001b[36m(_ray_fit pid=19487)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=19673)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 29)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=19741)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 28)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5495\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t87.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 8.89s of remaining time.\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tEnsemble Weights: {'LightGBM_r130_BAG_L2': 0.875, 'LightGBM_r131_BAG_L2': 0.062, 'CatBoost_r9_BAG_L2': 0.062}\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.5592\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t5.41s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=19743)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 28)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m AutoGluon training complete, total runtime = 7192.26s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 253.5 rows/s (3869 batch size)\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Calibrating decision threshold to optimize metric mcc | Checking 51 thresholds...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tBase Threshold: 0.500\t| val: 0.5592\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m \tBest Threshold: 0.500\t| val: 0.5592\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250106_060220/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=173)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                          model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           WeightedEnsemble_L3       0.555428   0.559234         mcc       19.764976      23.140816  4426.812824                 0.002270                0.004135           5.409113            3       True         84\n",
      "1          LightGBM_r130_BAG_L2       0.555394   0.558758         mcc       19.543513      22.707477  4367.647242                 0.078340                0.147317          24.308404            2       True         82\n",
      "2          LightGBM_r188_BAG_L1       0.553650   0.540528         mcc        0.492685       1.135163    33.584664                 0.492685                1.135163          33.584664            1       True         27\n",
      "3   NeuralNetFastAI_r191_BAG_L1       0.553032   0.534719         mcc        0.635512       0.399659   178.159765                 0.635512                0.399659         178.159765            1       True         17\n",
      "4       RandomForestGini_BAG_L2       0.553006   0.540848         mcc       19.867236      23.733752  4371.890790                 0.402063                1.173593          28.551952            2       True         56\n",
      "5      RandomForest_r195_BAG_L2       0.552131   0.539357         mcc       19.813953      23.703102  4530.882166                 0.348780                1.142942         187.543328            2       True         77\n",
      "6   NeuralNetFastAI_r102_BAG_L1       0.551451   0.537237         mcc        0.152961       0.121712    35.396355                 0.152961                0.121712          35.396355            1       True         24\n",
      "7                XGBoost_BAG_L2       0.550771   0.556452         mcc       19.613445      22.686300  4357.085686                 0.148272                0.126141          13.746848            2       True         62\n",
      "8            CatBoost_r9_BAG_L2       0.550704   0.547906         mcc       19.570923      22.774596  4371.302488                 0.105750                0.214436          27.963650            2       True         69\n",
      "9               LightGBM_BAG_L2       0.550609   0.556371         mcc       19.546533      22.724367  4365.323735                 0.081360                0.164207          21.984897            2       True         55\n",
      "10   NeuralNetFastAI_r95_BAG_L1       0.550407   0.536724         mcc        0.958218       0.721348   276.340466                 0.958218                0.721348         276.340466            1       True         47\n",
      "11           XGBoost_r89_BAG_L2       0.550071   0.554500         mcc       19.601755      22.712738  4356.053280                 0.136582                0.152578          12.714442            2       True         80\n",
      "12            LightGBMXT_BAG_L1       0.550027   0.538624         mcc        0.203942       0.537644    27.601486                 0.203942                0.537644          27.601486            1       True          3\n",
      "13            LightGBMXT_BAG_L2       0.549289   0.553157         mcc       19.579633      22.813615  4363.658851                 0.114460                0.253455          20.320013            2       True         54\n",
      "14        NeuralNetTorch_BAG_L1       0.549185   0.542159         mcc        0.176491       0.119557   177.639615                 0.176491                0.119557         177.639615            1       True         12\n",
      "15         LightGBM_r131_BAG_L2       0.549152   0.555126         mcc       19.578615      22.774928  4369.131657                 0.113442                0.214768          25.792819            2       True         67\n",
      "16    NeuralNetTorch_r86_BAG_L2       0.549088   0.549458         mcc       20.074245      22.723736  4430.896397                 0.609072                0.163577          87.557559            2       True         83\n",
      "17    NeuralNetTorch_r86_BAG_L1       0.548351   0.546502         mcc        0.753220       0.124956   180.656300                 0.753220                0.124956         180.656300            1       True         32\n",
      "18          CatBoost_r13_BAG_L2       0.548173   0.550538         mcc       19.522364      22.663315  4581.670814                 0.057191                0.103156         238.331976            2       True         76\n",
      "19           XGBoost_r33_BAG_L2       0.548173   0.556898         mcc       19.820169      22.719633  4368.224156                 0.354996                0.159473          24.885318            2       True         72\n",
      "20         CatBoost_r137_BAG_L2       0.548139   0.550821         mcc       19.501683      22.655852  4388.065503                 0.036510                0.095692          44.726665            2       True         74\n",
      "21    NeuralNetTorch_r22_BAG_L2       0.547921   0.549608         mcc       19.660371      22.710227  4471.377383                 0.195198                0.150067         128.038544            2       True         71\n",
      "22              CatBoost_BAG_L2       0.547736   0.551557         mcc       19.506577      22.660860  4396.391244                 0.041404                0.100700          53.052406            2       True         58\n",
      "23         CatBoost_r177_BAG_L2       0.547667   0.550474         mcc       19.518296      22.656255  4385.900709                 0.053123                0.096095          42.561871            2       True         65\n",
      "24         LightGBM_r188_BAG_L2       0.547496   0.551461         mcc       19.611111      22.825758  4370.323766                 0.145938                0.265599          26.984928            2       True         78\n",
      "25  NeuralNetFastAI_r145_BAG_L1       0.547472   0.537838         mcc        0.863608       0.764105   269.413635                 0.863608                0.764105         269.413635            1       True         28\n",
      "26       NeuralNetFastAI_BAG_L1       0.547126   0.538213         mcc        2.562706       0.425418   133.149115                 2.562706                0.425418         133.149115            1       True         10\n",
      "27  NeuralNetFastAI_r103_BAG_L1       0.546858   0.532652         mcc        0.518967       0.391911   163.736228                 0.518967                0.391911         163.736228            1       True         38\n",
      "28    NeuralNetTorch_r79_BAG_L2       0.546833   0.548415         mcc       19.659099      22.727631  4557.837004                 0.193926                0.167472         214.498166            2       True         66\n",
      "29  NeuralNetFastAI_r102_BAG_L2       0.546819   0.542343         mcc       19.636598      22.703546  4381.206645                 0.171425                0.143386          37.867807            2       True         75\n",
      "30         LightGBMLarge_BAG_L2       0.546587   0.555981         mcc       19.602114      22.758559  4378.706910                 0.136941                0.198399          35.368072            2       True         64\n",
      "31    NeuralNetTorch_r22_BAG_L1       0.546256   0.546827         mcc        0.195075       0.114836   199.381781                 0.195075                0.114836         199.381781            1       True         20\n",
      "32        ExtraTreesGini_BAG_L2       0.546114   0.541324         mcc       20.068804      23.869584  4347.731176                 0.603631                1.309424           4.392338            2       True         59\n",
      "33  NeuralNetFastAI_r156_BAG_L1       0.545875   0.536452         mcc        0.179964       0.115080    41.611694                 0.179964                0.115080          41.611694            1       True         43\n",
      "34    NeuralNetTorch_r79_BAG_L1       0.545860   0.545215         mcc        0.186239       0.135629   254.796364                 0.186239                0.135629         254.796364            1       True         15\n",
      "35      RandomForestEntr_BAG_L2       0.545540   0.543990         mcc       19.827283      23.724801  4379.810030                 0.362110                1.164641          36.471192            2       True         57\n",
      "36         LightGBM_r196_BAG_L1       0.545331   0.535931         mcc        3.526724       9.977756   116.752877                 3.526724                9.977756         116.752877            1       True         44\n",
      "37          WeightedEnsemble_L2       0.545102   0.547781         mcc        3.560446       2.261348  2322.125252                 0.006392                0.004320           4.728036            2       True         53\n",
      "38    NeuralNetTorch_r14_BAG_L1       0.544905   0.542498         mcc        0.096464       0.111588    82.771698                 0.096464                0.111588          82.771698            1       True         39\n",
      "39    NeuralNetTorch_r30_BAG_L2       0.544432   0.547160         mcc       19.715733      22.726763  4525.456623                 0.250560                0.166603         182.117785            2       True         81\n",
      "40          LightGBM_r96_BAG_L2       0.544186   0.546763         mcc       19.525303      22.700368  4361.278940                 0.060130                0.140209          17.940102            2       True         70\n",
      "41       NeuralNetFastAI_BAG_L2       0.544126   0.543330         mcc       19.965105      23.030982  4476.146379                 0.499932                0.470823         132.807540            2       True         61\n",
      "42       ExtraTrees_r172_BAG_L1       0.543997   0.536528         mcc        0.348123       0.932549     3.537771                 0.348123                0.932549           3.537771            1       True         36\n",
      "43  NeuralNetFastAI_r191_BAG_L2       0.543585   0.543827         mcc       20.106780      22.997723  4515.733248                 0.641607                0.437563         172.394410            2       True         68\n",
      "44  NeuralNetFastAI_r145_BAG_L2       0.543425   0.546267         mcc       20.365294      23.396734  4611.688784                 0.900120                0.836575         268.349946            2       True         79\n",
      "45          LightGBM_r96_BAG_L1       0.542764   0.536775         mcc        1.098799       3.095220    46.172266                 1.098799                3.095220          46.172266            1       True         19\n",
      "46    NeuralNetTorch_r41_BAG_L1       0.542000   0.545293         mcc        0.131634       0.143399   302.577929                 0.131634                0.143399         302.577929            1       True         48\n",
      "47        ExtraTreesEntr_BAG_L2       0.541456   0.539468         mcc       20.074396      23.851089  4348.570851                 0.609223                1.290929           5.232013            2       True         60\n",
      "48          CatBoost_r13_BAG_L1       0.541260   0.535152         mcc        0.058768       0.052868   401.753097                 0.058768                0.052868         401.753097            1       True         25\n",
      "49    NeuralNetTorch_r30_BAG_L1       0.541114   0.543321         mcc        0.223753       0.147994   312.600057                 0.223753                0.147994         312.600057            1       True         30\n",
      "50        NeuralNetTorch_BAG_L2       0.540985   0.549524         mcc       19.652130      22.706552  4480.147143                 0.186957                0.146392         136.808305            2       True         63\n",
      "51          LightGBM_r15_BAG_L1       0.540184   0.538785         mcc        0.555799       1.347807    31.774818                 0.555799                1.347807          31.774818            1       True         50\n",
      "52              CatBoost_BAG_L1       0.540125   0.533954         mcc        0.311613       0.034915    91.730213                 0.311613                0.034915          91.730213            1       True          7\n",
      "53        ExtraTrees_r42_BAG_L2       0.539231   0.539030         mcc       19.921947      23.759983  4358.977211                 0.456774                1.199823          15.638373            2       True         73\n",
      "54          CatBoost_r86_BAG_L1       0.538064   0.536415         mcc        0.035491       0.040163   134.788031                 0.035491                0.040163         134.788031            1       True         52\n",
      "55   NeuralNetTorch_r158_BAG_L1       0.537567   0.539203         mcc        0.668249       0.139906   248.383794                 0.668249                0.139906         248.383794            1       True         51\n",
      "56           XGBoost_r89_BAG_L1       0.537527   0.536636         mcc        0.170309       0.059993    14.913368                 0.170309                0.059993          14.913368            1       True         29\n",
      "57         CatBoost_r167_BAG_L1       0.537139   0.536626         mcc        0.035821       0.031376    93.333090                 0.035821                0.031376          93.333090            1       True         46\n",
      "58          CatBoost_r70_BAG_L1       0.537139   0.536124         mcc        0.125355       0.180208    25.718576                 0.125355                0.180208          25.718576            1       True         42\n",
      "59         LightGBM_r161_BAG_L1       0.536797   0.533718         mcc        0.473283       0.800259    46.148985                 0.473283                0.800259          46.148985            1       True         40\n",
      "60         CatBoost_r177_BAG_L1       0.536570   0.535106         mcc        0.035251       0.030221    75.540618                 0.035251                0.030221          75.540618            1       True         14\n",
      "61  NeuralNetFastAI_r143_BAG_L1       0.536482   0.531894         mcc        0.203516       0.182260    61.776900                 0.203516                0.182260          61.776900            1       True         41\n",
      "62           CatBoost_r9_BAG_L1       0.536002   0.534653         mcc        0.108455       0.175458    28.884139                 0.108455                0.175458          28.884139            1       True         18\n",
      "63         CatBoost_r137_BAG_L1       0.535973   0.533980         mcc        0.043849       0.039680    98.292395                 0.043849                0.039680          98.292395            1       True         23\n",
      "64          CatBoost_r50_BAG_L1       0.535944   0.534774         mcc        0.104464       0.193633    22.818570                 0.104464                0.193633          22.818570            1       True         33\n",
      "65          XGBoost_r194_BAG_L1       0.535889   0.538602         mcc        0.111127       0.053643    13.828002                 0.111127                0.053643          13.828002            1       True         35\n",
      "66           XGBoost_r98_BAG_L1       0.535331   0.530562         mcc        0.317963       0.093310    22.361917                 0.317963                0.093310          22.361917            1       True         49\n",
      "67         LightGBM_r131_BAG_L1       0.534986   0.536910         mcc        0.328045       0.657749    27.754966                 0.328045                0.657749          27.754966            1       True         16\n",
      "68      RandomForestEntr_BAG_L1       0.534269   0.514992         mcc        0.568238       1.072321     7.250917                 0.568238                1.072321           7.250917            1       True          6\n",
      "69   NeuralNetFastAI_r11_BAG_L1       0.534064   0.533961         mcc        0.922958       0.738920   239.791384                 0.922958                0.738920         239.791384            1       True         34\n",
      "70         LightGBMLarge_BAG_L1       0.533462   0.536135         mcc        0.143455       0.225771    24.908208                 0.143455                0.225771          24.908208            1       True         13\n",
      "71           XGBoost_r33_BAG_L1       0.533330   0.536770         mcc        0.376148       0.080362    19.626538                 0.376148                0.080362          19.626538            1       True         21\n",
      "72          CatBoost_r69_BAG_L1       0.532328   0.534355         mcc        0.039873       0.040189    91.519912                 0.039873                0.040189          91.519912            1       True         37\n",
      "73        ExtraTreesGini_BAG_L1       0.532314   0.525586         mcc        1.022153       1.260320     3.166367                 1.022153                1.260320           3.166367            1       True          8\n",
      "74               XGBoost_BAG_L1       0.530638   0.539416         mcc        0.156348       0.049545    13.265819                 0.156348                0.049545          13.265819            1       True         11\n",
      "75        ExtraTreesEntr_BAG_L1       0.529730   0.520025         mcc        1.007967       1.257692     3.089606                 1.007967                1.257692           3.089606            1       True          9\n",
      "76              LightGBM_BAG_L1       0.529619   0.537816         mcc        0.132731       0.323860    18.921610                 0.132731                0.323860          18.921610            1       True          4\n",
      "77      RandomForestGini_BAG_L1       0.527005   0.511864         mcc        0.593103       1.097635     6.154829                 0.593103                1.097635           6.154829            1       True          5\n",
      "78         LightGBM_r130_BAG_L1       0.525746   0.538773         mcc        0.214792       0.437970    23.407474                 0.214792                0.437970          23.407474            1       True         31\n",
      "79      RandomForest_r39_BAG_L1       0.525423   0.515989         mcc        0.503357       0.969468    13.167148                 0.503357                0.969468          13.167148            1       True         45\n",
      "80        ExtraTrees_r42_BAG_L1       0.522903   0.515833         mcc        0.813889       1.159208     4.221054                 0.813889                1.159208           4.221054            1       True         22\n",
      "81     RandomForest_r195_BAG_L1       0.518668   0.511142         mcc        0.492712       1.001426    13.213129                 0.492712                1.001426          13.213129            1       True         26\n",
      "82        KNeighborsUnif_BAG_L1       0.424624   0.432776         mcc        0.024937       0.125002     0.606830                 0.024937                0.125002           0.606830            1       True          1\n",
      "83        KNeighborsDist_BAG_L1       0.415326   0.424743         mcc        0.019054       0.126367     0.047837                 0.019054                0.126367           0.047837            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t7232s\t = DyStack   runtime |\t21568s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 21568s\n",
      "AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250106_060220\"\n",
      "Train Data Rows:    34819\n",
      "Train Data Columns: 12\n",
      "Label Column:       연체여부\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29607.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.19 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 8 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['소비 빈도 및 활동성', '소비 금액 및 규모', '연체 가능성 및 신용 리스크', '서비스 이용 다양성']\n",
      "\t\t('int', [])   : 8 | ['남성', '여성', '20대', '30대', '40대', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 4 | ['소비 빈도 및 활동성', '소비 금액 및 규모', '연체 가능성 및 신용 리스크', '서비스 이용 다양성']\n",
      "\t\t('int', ['bool']) : 8 | ['남성', '여성', '20대', '30대', '40대', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.33 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 14374.98s of the 21567.85s of remaining time.\n",
      "\t0.4304\t = Validation score   (mcc)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 14374.49s of the 21567.36s of remaining time.\n",
      "\t0.4245\t = Validation score   (mcc)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 14374.23s of the 21567.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\t0.543\t = Validation score   (mcc)\n",
      "\t26.57s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 14344.88s of the 21537.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\t0.5375\t = Validation score   (mcc)\n",
      "\t19.19s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 14323.85s of the 21516.71s of remaining time.\n",
      "\t0.5177\t = Validation score   (mcc)\n",
      "\t7.33s\t = Training   runtime\n",
      "\t1.32s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 14314.53s of the 21507.40s of remaining time.\n",
      "\t0.5164\t = Validation score   (mcc)\n",
      "\t8.45s\t = Training   runtime\n",
      "\t1.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 14304.11s of the 21496.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\t0.5357\t = Validation score   (mcc)\n",
      "\t97.17s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 14205.28s of the 21398.15s of remaining time.\n",
      "\t0.5214\t = Validation score   (mcc)\n",
      "\t3.64s\t = Training   runtime\n",
      "\t1.55s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 14198.92s of the 21391.78s of remaining time.\n",
      "\t0.5215\t = Validation score   (mcc)\n",
      "\t3.53s\t = Training   runtime\n",
      "\t1.58s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 14192.71s of the 21385.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5383\t = Validation score   (mcc)\n",
      "\t146.63s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 14044.30s of the 21237.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5397\t = Validation score   (mcc)\n",
      "\t12.49s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 14029.95s of the 21222.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5468\t = Validation score   (mcc)\n",
      "\t243.06s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 13785.06s of the 20977.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.5375\t = Validation score   (mcc)\n",
      "\t27.11s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 13755.84s of the 20948.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\t0.5345\t = Validation score   (mcc)\n",
      "\t69.27s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 13684.89s of the 20877.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.545\t = Validation score   (mcc)\n",
      "\t222.0s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 13460.81s of the 20653.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\t0.5366\t = Validation score   (mcc)\n",
      "\t29.8s\t = Training   runtime\n",
      "\t0.94s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 13428.80s of the 20621.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5389\t = Validation score   (mcc)\n",
      "\t199.22s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 13227.80s of the 20420.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.09%)\n",
      "\t0.5368\t = Validation score   (mcc)\n",
      "\t29.73s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 13196.01s of the 20388.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5416\t = Validation score   (mcc)\n",
      "\t52.69s\t = Training   runtime\n",
      "\t3.61s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 13141.02s of the 20333.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5448\t = Validation score   (mcc)\n",
      "\t267.19s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 12872.07s of the 20064.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.16%)\n",
      "\t0.5376\t = Validation score   (mcc)\n",
      "\t18.69s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 12851.45s of the 20044.31s of remaining time.\n",
      "\t0.5141\t = Validation score   (mcc)\n",
      "\t4.8s\t = Training   runtime\n",
      "\t1.43s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 12844.31s of the 20037.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5343\t = Validation score   (mcc)\n",
      "\t87.1s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 12755.54s of the 19948.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5378\t = Validation score   (mcc)\n",
      "\t37.27s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 12716.40s of the 19909.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.09%)\n",
      "\t0.5373\t = Validation score   (mcc)\n",
      "\t370.72s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 12343.65s of the 19536.52s of remaining time.\n",
      "\t0.5106\t = Validation score   (mcc)\n",
      "\t14.68s\t = Training   runtime\n",
      "\t1.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 12327.25s of the 19520.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.543\t = Validation score   (mcc)\n",
      "\t34.19s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 12291.22s of the 19484.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5399\t = Validation score   (mcc)\n",
      "\t295.57s\t = Training   runtime\n",
      "\t0.95s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 11993.60s of the 19186.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.5403\t = Validation score   (mcc)\n",
      "\t15.73s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 11975.45s of the 19168.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5423\t = Validation score   (mcc)\n",
      "\t218.89s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 11754.84s of the 18947.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5378\t = Validation score   (mcc)\n",
      "\t22.92s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 11729.87s of the 18922.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5459\t = Validation score   (mcc)\n",
      "\t218.77s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 11509.31s of the 18702.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5379\t = Validation score   (mcc)\n",
      "\t24.64s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 11482.81s of the 18675.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5333\t = Validation score   (mcc)\n",
      "\t269.88s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 11210.92s of the 18403.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.07%)\n",
      "\t0.539\t = Validation score   (mcc)\n",
      "\t12.47s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 11196.57s of the 18389.43s of remaining time.\n",
      "\t0.5371\t = Validation score   (mcc)\n",
      "\t4.21s\t = Training   runtime\n",
      "\t1.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 11190.72s of the 18383.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\t0.5351\t = Validation score   (mcc)\n",
      "\t75.11s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 11113.98s of the 18306.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5352\t = Validation score   (mcc)\n",
      "\t185.59s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 10926.49s of the 18119.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5469\t = Validation score   (mcc)\n",
      "\t236.68s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 10687.86s of the 17880.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.09%)\n",
      "\t0.5352\t = Validation score   (mcc)\n",
      "\t45.79s\t = Training   runtime\n",
      "\t0.9s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 10639.51s of the 17832.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5312\t = Validation score   (mcc)\n",
      "\t66.0s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 10571.62s of the 17764.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5384\t = Validation score   (mcc)\n",
      "\t24.47s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 10545.06s of the 17737.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5401\t = Validation score   (mcc)\n",
      "\t44.02s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 10499.23s of the 17692.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.5404\t = Validation score   (mcc)\n",
      "\t132.68s\t = Training   runtime\n",
      "\t13.36s\t = Validation runtime\n",
      "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 10363.06s of the 17555.93s of remaining time.\n",
      "\t0.5147\t = Validation score   (mcc)\n",
      "\t13.91s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 10347.44s of the 17540.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.5355\t = Validation score   (mcc)\n",
      "\t84.91s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 10260.88s of the 17453.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5372\t = Validation score   (mcc)\n",
      "\t304.55s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 9954.32s of the 17147.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.545\t = Validation score   (mcc)\n",
      "\t317.72s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 9634.60s of the 16827.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.10%)\n",
      "\t0.5327\t = Validation score   (mcc)\n",
      "\t24.63s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_r15_BAG_L1 ... Training model for up to 9607.84s of the 16800.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.536\t = Validation score   (mcc)\n",
      "\t25.53s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 9580.40s of the 16773.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5384\t = Validation score   (mcc)\n",
      "\t253.77s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_r86_BAG_L1 ... Training model for up to 9324.89s of the 16517.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.09%)\n",
      "\t0.5376\t = Validation score   (mcc)\n",
      "\t217.43s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 9105.52s of the 16298.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5366\t = Validation score   (mcc)\n",
      "\t113.33s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 8990.25s of the 16183.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5412\t = Validation score   (mcc)\n",
      "\t89.2s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_r49_BAG_L1 ... Training model for up to 8899.23s of the 16092.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5345\t = Validation score   (mcc)\n",
      "\t78.28s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 8818.87s of the 16011.74s of remaining time.\n",
      "\t0.5214\t = Validation score   (mcc)\n",
      "\t3.7s\t = Training   runtime\n",
      "\t1.55s\t = Validation runtime\n",
      "Fitting model: LightGBM_r143_BAG_L1 ... Training model for up to 8812.62s of the 16005.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.07%)\n",
      "\t0.5363\t = Validation score   (mcc)\n",
      "\t35.18s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Fitting model: RandomForest_r127_BAG_L1 ... Training model for up to 8775.27s of the 15968.14s of remaining time.\n",
      "\t0.5188\t = Validation score   (mcc)\n",
      "\t16.95s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 8756.95s of the 15949.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5389\t = Validation score   (mcc)\n",
      "\t48.6s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: RandomForest_r34_BAG_L1 ... Training model for up to 8706.69s of the 15899.56s of remaining time.\n",
      "\t0.5316\t = Validation score   (mcc)\n",
      "\t10.18s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "Fitting model: LightGBM_r94_BAG_L1 ... Training model for up to 8695.46s of the 15888.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.539\t = Validation score   (mcc)\n",
      "\t40.57s\t = Training   runtime\n",
      "\t1.93s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 8652.36s of the 15845.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5473\t = Validation score   (mcc)\n",
      "\t335.79s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r128_BAG_L1 ... Training model for up to 8314.69s of the 15507.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.09%)\n",
      "\t0.5331\t = Validation score   (mcc)\n",
      "\t26.02s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 8286.96s of the 15479.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5345\t = Validation score   (mcc)\n",
      "\t32.54s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 8252.60s of the 15445.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5412\t = Validation score   (mcc)\n",
      "\t159.19s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 8091.27s of the 15284.14s of remaining time.\n",
      "\t0.5366\t = Validation score   (mcc)\n",
      "\t3.08s\t = Training   runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r65_BAG_L1 ... Training model for up to 8087.08s of the 15279.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5368\t = Validation score   (mcc)\n",
      "\t58.12s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r88_BAG_L1 ... Training model for up to 8027.27s of the 15220.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5339\t = Validation score   (mcc)\n",
      "\t57.31s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBM_r30_BAG_L1 ... Training model for up to 7967.91s of the 15160.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.5388\t = Validation score   (mcc)\n",
      "\t57.6s\t = Training   runtime\n",
      "\t3.69s\t = Validation runtime\n",
      "Fitting model: XGBoost_r49_BAG_L1 ... Training model for up to 7906.82s of the 15099.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.07%)\n",
      "\t0.54\t = Validation score   (mcc)\n",
      "\t17.45s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r5_BAG_L1 ... Training model for up to 7887.70s of the 15080.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5334\t = Validation score   (mcc)\n",
      "\t78.87s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r87_BAG_L1 ... Training model for up to 7807.15s of the 15000.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5441\t = Validation score   (mcc)\n",
      "\t376.37s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r71_BAG_L1 ... Training model for up to 7428.99s of the 14621.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5462\t = Validation score   (mcc)\n",
      "\t161.85s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost_r143_BAG_L1 ... Training model for up to 7265.39s of the 14458.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.5345\t = Validation score   (mcc)\n",
      "\t80.0s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r178_BAG_L1 ... Training model for up to 7183.63s of the 14376.50s of remaining time.\n",
      "\t0.5382\t = Validation score   (mcc)\n",
      "\t3.35s\t = Training   runtime\n",
      "\t1.13s\t = Validation runtime\n",
      "Fitting model: RandomForest_r166_BAG_L1 ... Training model for up to 7178.86s of the 14371.73s of remaining time.\n",
      "\t0.5177\t = Validation score   (mcc)\n",
      "\t7.23s\t = Training   runtime\n",
      "\t1.33s\t = Validation runtime\n",
      "Fitting model: XGBoost_r31_BAG_L1 ... Training model for up to 7169.82s of the 14362.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5208\t = Validation score   (mcc)\n",
      "\t24.14s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r185_BAG_L1 ... Training model for up to 7143.98s of the 14336.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5451\t = Validation score   (mcc)\n",
      "\t185.56s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r160_BAG_L1 ... Training model for up to 6956.54s of the 14149.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5327\t = Validation score   (mcc)\n",
      "\t200.72s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: CatBoost_r60_BAG_L1 ... Training model for up to 6753.88s of the 13946.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\t0.5346\t = Validation score   (mcc)\n",
      "\t81.15s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForest_r15_BAG_L1 ... Training model for up to 6670.85s of the 13863.72s of remaining time.\n",
      "\t0.517\t = Validation score   (mcc)\n",
      "\t14.13s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "Fitting model: LightGBM_r135_BAG_L1 ... Training model for up to 6655.23s of the 13848.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.08%)\n",
      "\t0.535\t = Validation score   (mcc)\n",
      "\t25.46s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: XGBoost_r22_BAG_L1 ... Training model for up to 6628.12s of the 13820.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5385\t = Validation score   (mcc)\n",
      "\t13.09s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r69_BAG_L1 ... Training model for up to 6613.27s of the 13806.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5354\t = Validation score   (mcc)\n",
      "\t209.07s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: CatBoost_r6_BAG_L1 ... Training model for up to 6402.27s of the 13595.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5381\t = Validation score   (mcc)\n",
      "\t22.12s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r138_BAG_L1 ... Training model for up to 6378.00s of the 13570.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5383\t = Validation score   (mcc)\n",
      "\t384.07s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "Fitting model: LightGBM_r121_BAG_L1 ... Training model for up to 5991.81s of the 13184.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.08%)\n",
      "\t0.5364\t = Validation score   (mcc)\n",
      "\t45.94s\t = Training   runtime\n",
      "\t0.97s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r172_BAG_L1 ... Training model for up to 5943.31s of the 13136.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5363\t = Validation score   (mcc)\n",
      "\t80.33s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: CatBoost_r180_BAG_L1 ... Training model for up to 5861.07s of the 13053.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.5346\t = Validation score   (mcc)\n",
      "\t22.49s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r76_BAG_L1 ... Training model for up to 5836.54s of the 13029.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5427\t = Validation score   (mcc)\n",
      "\t115.36s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r197_BAG_L1 ... Training model for up to 5719.34s of the 12912.20s of remaining time.\n",
      "\t0.5111\t = Validation score   (mcc)\n",
      "\t5.77s\t = Training   runtime\n",
      "\t1.43s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r121_BAG_L1 ... Training model for up to 5711.60s of the 12904.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5428\t = Validation score   (mcc)\n",
      "\t212.0s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r127_BAG_L1 ... Training model for up to 5497.90s of the 12690.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5372\t = Validation score   (mcc)\n",
      "\t48.33s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForest_r16_BAG_L1 ... Training model for up to 5447.63s of the 12640.49s of remaining time.\n",
      "\t0.5041\t = Validation score   (mcc)\n",
      "\t19.26s\t = Training   runtime\n",
      "\t1.28s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r194_BAG_L1 ... Training model for up to 5426.68s of the 12619.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5387\t = Validation score   (mcc)\n",
      "\t116.23s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: CatBoost_r12_BAG_L1 ... Training model for up to 5308.73s of the 12501.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.06%)\n",
      "\t0.5361\t = Validation score   (mcc)\n",
      "\t156.0s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r135_BAG_L1 ... Training model for up to 5150.76s of the 12343.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5456\t = Validation score   (mcc)\n",
      "\t401.25s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r4_BAG_L1 ... Training model for up to 4747.68s of the 11940.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5302\t = Validation score   (mcc)\n",
      "\t148.05s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r126_BAG_L1 ... Training model for up to 4597.80s of the 11790.67s of remaining time.\n",
      "\t0.5343\t = Validation score   (mcc)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t1.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r36_BAG_L1 ... Training model for up to 4594.21s of the 11787.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5426\t = Validation score   (mcc)\n",
      "\t224.9s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r100_BAG_L1 ... Training model for up to 4367.64s of the 11560.50s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5386\t = Validation score   (mcc)\n",
      "\t43.36s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r163_BAG_L1 ... Training model for up to 4322.04s of the 11514.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.04%)\n",
      "\t0.5349\t = Validation score   (mcc)\n",
      "\t58.86s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost_r198_BAG_L1 ... Training model for up to 4261.01s of the 11453.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5362\t = Validation score   (mcc)\n",
      "\t108.74s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r187_BAG_L1 ... Training model for up to 4150.39s of the 11343.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5407\t = Validation score   (mcc)\n",
      "\t71.38s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r19_BAG_L1 ... Training model for up to 4077.13s of the 11269.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5428\t = Validation score   (mcc)\n",
      "\t121.9s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost_r95_BAG_L1 ... Training model for up to 3953.17s of the 11146.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.05%)\n",
      "\t0.5373\t = Validation score   (mcc)\n",
      "\t15.2s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost_r34_BAG_L1 ... Training model for up to 3935.87s of the 11128.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.16%)\n",
      "\t0.536\t = Validation score   (mcc)\n",
      "\t17.02s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_r42_BAG_L1 ... Training model for up to 3917.10s of the 11109.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.08%)\n",
      "\t0.5421\t = Validation score   (mcc)\n",
      "\t25.89s\t = Training   runtime\n",
      "\t1.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r1_BAG_L1 ... Training model for up to 3889.39s of the 11082.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5454\t = Validation score   (mcc)\n",
      "\t445.76s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r89_BAG_L1 ... Training model for up to 3441.78s of the 10634.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.03%)\n",
      "\t0.5436\t = Validation score   (mcc)\n",
      "\t194.83s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1437.50s of the 10437.80s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_r143_BAG_L1': 0.913, 'NeuralNetTorch_r86_BAG_L1': 0.043, 'NeuralNetTorch_r135_BAG_L1': 0.043}\n",
      "\t0.5476\t = Validation score   (mcc)\n",
      "\t6.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 10431.62s of the 10431.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.43%)\n",
      "\t0.5506\t = Validation score   (mcc)\n",
      "\t22.15s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 10407.59s of the 10407.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.43%)\n",
      "\t0.5546\t = Validation score   (mcc)\n",
      "\t20.45s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 10385.19s of the 10384.93s of remaining time.\n",
      "\t0.5407\t = Validation score   (mcc)\n",
      "\t38.76s\t = Training   runtime\n",
      "\t1.62s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 10344.38s of the 10344.11s of remaining time.\n",
      "\t0.5446\t = Validation score   (mcc)\n",
      "\t48.54s\t = Training   runtime\n",
      "\t1.59s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 10293.83s of the 10293.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.51%)\n",
      "\t0.5485\t = Validation score   (mcc)\n",
      "\t64.75s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 10227.25s of the 10226.98s of remaining time.\n",
      "\t0.5388\t = Validation score   (mcc)\n",
      "\t5.27s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 10219.54s of the 10219.28s of remaining time.\n",
      "\t0.5425\t = Validation score   (mcc)\n",
      "\t5.81s\t = Training   runtime\n",
      "\t1.67s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 10211.46s of the 10211.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5446\t = Validation score   (mcc)\n",
      "\t146.85s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 10062.71s of the 10062.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.65%)\n",
      "\t0.5558\t = Validation score   (mcc)\n",
      "\t15.03s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 10045.32s of the 10045.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5492\t = Validation score   (mcc)\n",
      "\t136.55s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 9906.81s of the 9906.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.68%)\n",
      "\t0.5533\t = Validation score   (mcc)\n",
      "\t41.77s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 9863.05s of the 9862.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.51%)\n",
      "\t0.5476\t = Validation score   (mcc)\n",
      "\t53.69s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 9807.36s of the 9807.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5505\t = Validation score   (mcc)\n",
      "\t233.94s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 9571.41s of the 9571.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.49%)\n",
      "\t0.5536\t = Validation score   (mcc)\n",
      "\t30.31s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 9539.07s of the 9538.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5453\t = Validation score   (mcc)\n",
      "\t204.64s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 9332.35s of the 9332.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.00%)\n",
      "\t0.5481\t = Validation score   (mcc)\n",
      "\t31.04s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 9299.36s of the 9299.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.40%)\n",
      "\t0.5469\t = Validation score   (mcc)\n",
      "\t19.94s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 9277.48s of the 9277.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5505\t = Validation score   (mcc)\n",
      "\t172.36s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 9103.22s of the 9102.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.74%)\n",
      "\t0.5529\t = Validation score   (mcc)\n",
      "\t27.24s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 9073.75s of the 9073.48s of remaining time.\n",
      "\t0.5431\t = Validation score   (mcc)\n",
      "\t21.0s\t = Training   runtime\n",
      "\t1.54s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 9050.73s of the 9050.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.39%)\n",
      "\t0.5482\t = Validation score   (mcc)\n",
      "\t53.73s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 8995.14s of the 8994.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.547\t = Validation score   (mcc)\n",
      "\t40.22s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 8952.83s of the 8952.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.02%)\n",
      "\t0.5498\t = Validation score   (mcc)\n",
      "\t248.7s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 8701.90s of the 8701.63s of remaining time.\n",
      "\t0.544\t = Validation score   (mcc)\n",
      "\t258.17s\t = Training   runtime\n",
      "\t1.49s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 8441.82s of the 8441.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "\t0.5527\t = Validation score   (mcc)\n",
      "\t32.6s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 8407.22s of the 8406.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5446\t = Validation score   (mcc)\n",
      "\t308.75s\t = Training   runtime\n",
      "\t0.99s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 8096.18s of the 8095.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.63%)\n",
      "\t0.5548\t = Validation score   (mcc)\n",
      "\t15.17s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L2 ... Training model for up to 8078.96s of the 8078.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5481\t = Validation score   (mcc)\n",
      "\t278.49s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L2 ... Training model for up to 7798.57s of the 7798.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.58%)\n",
      "\t0.5538\t = Validation score   (mcc)\n",
      "\t29.03s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L2 ... Training model for up to 7767.47s of the 7767.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5504\t = Validation score   (mcc)\n",
      "\t158.25s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L2 ... Training model for up to 7607.31s of the 7607.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.40%)\n",
      "\t0.5491\t = Validation score   (mcc)\n",
      "\t22.49s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L2 ... Training model for up to 7582.86s of the 7582.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5417\t = Validation score   (mcc)\n",
      "\t281.29s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194_BAG_L2 ... Training model for up to 7299.38s of the 7299.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.79%)\n",
      "\t0.5524\t = Validation score   (mcc)\n",
      "\t14.62s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L2 ... Training model for up to 7282.66s of the 7282.39s of remaining time.\n",
      "\t0.5438\t = Validation score   (mcc)\n",
      "\t22.1s\t = Training   runtime\n",
      "\t1.43s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L2 ... Training model for up to 7258.75s of the 7258.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.43%)\n",
      "\t0.549\t = Validation score   (mcc)\n",
      "\t49.88s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L2 ... Training model for up to 7207.02s of the 7206.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5417\t = Validation score   (mcc)\n",
      "\t190.5s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L2 ... Training model for up to 7014.45s of the 7014.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.55\t = Validation score   (mcc)\n",
      "\t158.52s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L2 ... Training model for up to 6853.88s of the 6853.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.99%)\n",
      "\t0.5527\t = Validation score   (mcc)\n",
      "\t56.31s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L2 ... Training model for up to 6795.51s of the 6795.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5411\t = Validation score   (mcc)\n",
      "\t71.64s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: CatBoost_r70_BAG_L2 ... Training model for up to 6721.92s of the 6721.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.52%)\n",
      "\t0.5498\t = Validation score   (mcc)\n",
      "\t25.32s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L2 ... Training model for up to 6694.56s of the 6694.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5415\t = Validation score   (mcc)\n",
      "\t46.23s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_r196_BAG_L2 ... Training model for up to 6646.34s of the 6646.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.71%)\n",
      "\t0.5505\t = Validation score   (mcc)\n",
      "\t43.32s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: RandomForest_r39_BAG_L2 ... Training model for up to 6599.83s of the 6599.56s of remaining time.\n",
      "\t0.5443\t = Validation score   (mcc)\n",
      "\t242.12s\t = Training   runtime\n",
      "\t1.49s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L2 ... Training model for up to 6355.80s of the 6355.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.66%)\n",
      "\t0.551\t = Validation score   (mcc)\n",
      "\t73.61s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r95_BAG_L2 ... Training model for up to 6280.26s of the 6280.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5445\t = Validation score   (mcc)\n",
      "\t315.04s\t = Training   runtime\n",
      "\t0.97s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r41_BAG_L2 ... Training model for up to 5963.06s of the 5962.80s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5499\t = Validation score   (mcc)\n",
      "\t279.31s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: XGBoost_r98_BAG_L2 ... Training model for up to 5681.42s of the 5681.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.11%)\n",
      "\t0.5548\t = Validation score   (mcc)\n",
      "\t22.17s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBM_r15_BAG_L2 ... Training model for up to 5657.25s of the 5656.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.41%)\n",
      "\t0.552\t = Validation score   (mcc)\n",
      "\t21.21s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r158_BAG_L2 ... Training model for up to 5634.16s of the 5633.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.544\t = Validation score   (mcc)\n",
      "\t169.05s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r86_BAG_L2 ... Training model for up to 5463.19s of the 5462.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.99%)\n",
      "\t0.5476\t = Validation score   (mcc)\n",
      "\t168.08s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r37_BAG_L2 ... Training model for up to 5293.06s of the 5292.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.542\t = Validation score   (mcc)\n",
      "\t113.42s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r197_BAG_L2 ... Training model for up to 5177.54s of the 5177.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5488\t = Validation score   (mcc)\n",
      "\t144.31s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r49_BAG_L2 ... Training model for up to 5031.24s of the 5030.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.40%)\n",
      "\t0.5487\t = Validation score   (mcc)\n",
      "\t46.83s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r49_BAG_L2 ... Training model for up to 4982.39s of the 4982.12s of remaining time.\n",
      "\t0.5388\t = Validation score   (mcc)\n",
      "\t5.19s\t = Training   runtime\n",
      "\t1.75s\t = Validation runtime\n",
      "Fitting model: LightGBM_r143_BAG_L2 ... Training model for up to 4974.82s of the 4974.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.78%)\n",
      "\t0.5494\t = Validation score   (mcc)\n",
      "\t49.99s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: RandomForest_r127_BAG_L2 ... Training model for up to 4922.68s of the 4922.41s of remaining time.\n",
      "\t0.5468\t = Validation score   (mcc)\n",
      "\t292.94s\t = Training   runtime\n",
      "\t1.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r134_BAG_L2 ... Training model for up to 4627.98s of the 4627.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5448\t = Validation score   (mcc)\n",
      "\t52.38s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: RandomForest_r34_BAG_L2 ... Training model for up to 4573.76s of the 4573.49s of remaining time.\n",
      "\t0.5494\t = Validation score   (mcc)\n",
      "\t144.4s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "Fitting model: LightGBM_r94_BAG_L2 ... Training model for up to 4427.90s of the 4427.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.39%)\n",
      "\t0.5473\t = Validation score   (mcc)\n",
      "\t21.61s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r143_BAG_L2 ... Training model for up to 4404.35s of the 4404.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5501\t = Validation score   (mcc)\n",
      "\t371.43s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_r128_BAG_L2 ... Training model for up to 4030.92s of the 4030.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.99%)\n",
      "\t0.5514\t = Validation score   (mcc)\n",
      "\t28.79s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r111_BAG_L2 ... Training model for up to 4000.14s of the 3999.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5402\t = Validation score   (mcc)\n",
      "\t35.57s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r31_BAG_L2 ... Training model for up to 3962.52s of the 3962.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.34%)\n",
      "\t0.5495\t = Validation score   (mcc)\n",
      "\t257.46s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r4_BAG_L2 ... Training model for up to 3702.82s of the 3702.55s of remaining time.\n",
      "\t0.5469\t = Validation score   (mcc)\n",
      "\t15.56s\t = Training   runtime\n",
      "\t1.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r65_BAG_L2 ... Training model for up to 3685.73s of the 3685.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5433\t = Validation score   (mcc)\n",
      "\t61.14s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r88_BAG_L2 ... Training model for up to 3622.69s of the 3622.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.63%)\n",
      "\t0.5418\t = Validation score   (mcc)\n",
      "\t72.4s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: LightGBM_r30_BAG_L2 ... Training model for up to 3548.07s of the 3547.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.64%)\n",
      "\t0.552\t = Validation score   (mcc)\n",
      "\t44.12s\t = Training   runtime\n",
      "\t1.32s\t = Validation runtime\n",
      "Fitting model: XGBoost_r49_BAG_L2 ... Training model for up to 3500.55s of the 3500.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.78%)\n",
      "\t0.5544\t = Validation score   (mcc)\n",
      "\t14.63s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: CatBoost_r5_BAG_L2 ... Training model for up to 3483.84s of the 3483.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.40%)\n",
      "\t0.5487\t = Validation score   (mcc)\n",
      "\t55.2s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r87_BAG_L2 ... Training model for up to 3426.68s of the 3426.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5467\t = Validation score   (mcc)\n",
      "\t348.55s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r71_BAG_L2 ... Training model for up to 3076.14s of the 3075.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5511\t = Validation score   (mcc)\n",
      "\t142.61s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r143_BAG_L2 ... Training model for up to 2931.37s of the 2931.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.68%)\n",
      "\t0.5482\t = Validation score   (mcc)\n",
      "\t75.48s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r178_BAG_L2 ... Training model for up to 2853.77s of the 2853.51s of remaining time.\n",
      "\t0.5478\t = Validation score   (mcc)\n",
      "\t16.4s\t = Training   runtime\n",
      "\t1.45s\t = Validation runtime\n",
      "Fitting model: RandomForest_r166_BAG_L2 ... Training model for up to 2835.49s of the 2835.22s of remaining time.\n",
      "\t0.5442\t = Validation score   (mcc)\n",
      "\t28.63s\t = Training   runtime\n",
      "\t1.53s\t = Validation runtime\n",
      "Fitting model: XGBoost_r31_BAG_L2 ... Training model for up to 2804.87s of the 2804.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.62%)\n",
      "\t0.5488\t = Validation score   (mcc)\n",
      "\t15.73s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r185_BAG_L2 ... Training model for up to 2787.26s of the 2786.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5496\t = Validation score   (mcc)\n",
      "\t259.92s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r160_BAG_L2 ... Training model for up to 2525.41s of the 2525.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.61%)\n",
      "\t0.541\t = Validation score   (mcc)\n",
      "\t205.86s\t = Training   runtime\n",
      "\t1.04s\t = Validation runtime\n",
      "Fitting model: CatBoost_r60_BAG_L2 ... Training model for up to 2317.38s of the 2317.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.45%)\n",
      "\t0.548\t = Validation score   (mcc)\n",
      "\t48.51s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForest_r15_BAG_L2 ... Training model for up to 2266.78s of the 2266.51s of remaining time.\n",
      "\t0.5474\t = Validation score   (mcc)\n",
      "\t229.6s\t = Training   runtime\n",
      "\t1.46s\t = Validation runtime\n",
      "Fitting model: LightGBM_r135_BAG_L2 ... Training model for up to 2035.29s of the 2035.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.86%)\n",
      "\t0.5532\t = Validation score   (mcc)\n",
      "\t38.53s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: XGBoost_r22_BAG_L2 ... Training model for up to 1994.83s of the 1994.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.63%)\n",
      "\t0.5548\t = Validation score   (mcc)\n",
      "\t14.05s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r69_BAG_L2 ... Training model for up to 1978.85s of the 1978.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.61%)\n",
      "\t0.5421\t = Validation score   (mcc)\n",
      "\t211.29s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Fitting model: CatBoost_r6_BAG_L2 ... Training model for up to 1765.54s of the 1765.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.41%)\n",
      "\t0.548\t = Validation score   (mcc)\n",
      "\t19.77s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r138_BAG_L2 ... Training model for up to 1743.45s of the 1743.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5466\t = Validation score   (mcc)\n",
      "\t370.87s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: LightGBM_r121_BAG_L2 ... Training model for up to 1370.30s of the 1370.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.90%)\n",
      "\t0.5529\t = Validation score   (mcc)\n",
      "\t55.13s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r172_BAG_L2 ... Training model for up to 1312.75s of the 1312.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5445\t = Validation score   (mcc)\n",
      "\t84.39s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_r180_BAG_L2 ... Training model for up to 1226.32s of the 1226.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.70%)\n",
      "\t0.5518\t = Validation score   (mcc)\n",
      "\t23.52s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r76_BAG_L2 ... Training model for up to 1200.56s of the 1200.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.546\t = Validation score   (mcc)\n",
      "\t133.39s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r197_BAG_L2 ... Training model for up to 1064.95s of the 1064.68s of remaining time.\n",
      "\t0.5438\t = Validation score   (mcc)\n",
      "\t27.73s\t = Training   runtime\n",
      "\t1.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r121_BAG_L2 ... Training model for up to 1035.18s of the 1034.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5494\t = Validation score   (mcc)\n",
      "\t302.0s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r127_BAG_L2 ... Training model for up to 731.33s of the 731.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.61%)\n",
      "\t0.5439\t = Validation score   (mcc)\n",
      "\t50.93s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: RandomForest_r16_BAG_L2 ... Training model for up to 678.45s of the 678.19s of remaining time.\n",
      "\t0.5405\t = Validation score   (mcc)\n",
      "\t351.33s\t = Training   runtime\n",
      "\t1.51s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r194_BAG_L2 ... Training model for up to 325.20s of the 324.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.60%)\n",
      "\t0.5432\t = Validation score   (mcc)\n",
      "\t120.83s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Fitting model: CatBoost_r12_BAG_L2 ... Training model for up to 202.41s of the 202.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.69%)\n",
      "\t0.5482\t = Validation score   (mcc)\n",
      "\t104.42s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r135_BAG_L2 ... Training model for up to 95.75s of the 95.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=0.33%)\n",
      "\t0.5475\t = Validation score   (mcc)\n",
      "\t87.17s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 1043.16s of the 6.08s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_BAG_L2': 0.95, 'XGBoost_r98_BAG_L2': 0.05}\n",
      "\t0.5558\t = Validation score   (mcc)\n",
      "\t5.95s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 21567.93s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 125.9 rows/s (4353 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric mcc | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.5558\n",
      "\tBest Threshold: 0.499\t| val: 0.5559\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.499\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.499 will be predicted as the positive class (1). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250106_060220\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                           model  score_val eval_metric  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0            WeightedEnsemble_L3   0.555835         mcc      52.445503  4827.681274                0.004437           5.946056            3       True        207\n",
      "1                 XGBoost_BAG_L2   0.555816         mcc      52.271777  4799.566803                0.158778          15.030980            2       True        120\n",
      "2             XGBoost_r22_BAG_L2   0.554833         mcc      52.279359  4798.582579                0.166360          14.046756            2       True        192\n",
      "3             XGBoost_r89_BAG_L2   0.554782         mcc      52.287435  4799.701518                0.174437          15.165695            2       True        138\n",
      "4             XGBoost_r98_BAG_L2   0.554763         mcc      52.282288  4806.704237                0.169289          22.168415            2       True        158\n",
      "5                LightGBM_BAG_L2   0.554568         mcc      52.261687  4804.990291                0.148688          20.454468            2       True        113\n",
      "6             XGBoost_r49_BAG_L2   0.554395         mcc      52.281121  4799.166835                0.168122          14.631012            2       True        179\n",
      "7           LightGBM_r130_BAG_L2   0.553849         mcc      52.405612  4813.562503                0.292613          29.026680            2       True        140\n",
      "8           LightGBM_r131_BAG_L2   0.553624         mcc      52.448596  4814.842742                0.335598          30.306919            2       True        125\n",
      "9           LightGBMLarge_BAG_L2   0.553276         mcc      52.429033  4826.307535                0.316035          41.771713            2       True        122\n",
      "10          LightGBM_r135_BAG_L2   0.553240         mcc      52.339049  4823.062715                0.226050          38.526892            2       True        191\n",
      "11            XGBoost_r33_BAG_L2   0.552947         mcc      52.297887  4811.773485                0.184889          27.237662            2       True        130\n",
      "12          LightGBM_r121_BAG_L2   0.552862         mcc      52.677534  4839.661898                0.564535          55.126076            2       True        196\n",
      "13          LightGBM_r188_BAG_L2   0.552709         mcc      52.709543  4817.133623                0.596544          32.597800            2       True        136\n",
      "14          LightGBM_r161_BAG_L2   0.552707         mcc      52.597044  4840.843611                0.484046          56.307789            2       True        149\n",
      "15           XGBoost_r194_BAG_L2   0.552440         mcc      52.262943  4799.158156                0.149944          14.622334            2       True        144\n",
      "16           LightGBM_r15_BAG_L2   0.552010         mcc      52.300197  4805.741169                0.187199          21.205346            2       True        159\n",
      "17           LightGBM_r30_BAG_L2   0.551987         mcc      53.432891  4828.653005                1.319893          44.117182            2       True        178\n",
      "18          CatBoost_r180_BAG_L2   0.551843         mcc      52.291375  4808.060462                0.178377          23.524639            2       True        198\n",
      "19          CatBoost_r128_BAG_L2   0.551407         mcc      52.399503  4813.327164                0.286505          28.791341            2       True        172\n",
      "20     NeuralNetTorch_r71_BAG_L2   0.551107         mcc      52.266851  4927.149724                0.153852         142.613902            2       True        182\n",
      "21          CatBoost_r167_BAG_L2   0.551019         mcc      52.263971  4858.150076                0.150972          73.614254            2       True        155\n",
      "22             LightGBMXT_BAG_L2   0.550650         mcc      52.464421  4806.686574                0.351423          22.150751            2       True        112\n",
      "23          LightGBM_r196_BAG_L2   0.550535         mcc      52.957773  4827.859184                0.844774          43.323361            2       True        153\n",
      "24     NeuralNetTorch_r79_BAG_L2   0.550526         mcc      52.301588  5018.471840                0.188589         233.936017            2       True        124\n",
      "25     NeuralNetTorch_r22_BAG_L2   0.550460         mcc      52.266427  4956.891752                0.153428         172.355930            2       True        129\n",
      "26     NeuralNetTorch_r86_BAG_L2   0.550408         mcc      52.287535  4942.786769                0.174537         158.250947            2       True        141\n",
      "27    NeuralNetTorch_r143_BAG_L2   0.550142         mcc      52.307786  5155.964507                0.194787         371.428684            2       True        171\n",
      "28     NeuralNetTorch_r14_BAG_L2   0.549985         mcc      52.268687  4943.058777                0.155689         158.522954            2       True        148\n",
      "29     NeuralNetTorch_r41_BAG_L2   0.549950         mcc      52.315407  5063.845011                0.202408         279.309188            2       True        157\n",
      "30           CatBoost_r13_BAG_L2   0.549783         mcc      52.235485  5033.235255                0.122486         248.699432            2       True        134\n",
      "31           CatBoost_r70_BAG_L2   0.549776         mcc      52.341854  4809.852209                0.228855          25.316386            2       True        151\n",
      "32    NeuralNetTorch_r185_BAG_L2   0.549648         mcc      52.305342  5044.454912                0.192343         259.919089            2       True        187\n",
      "33     NeuralNetTorch_r31_BAG_L2   0.549480         mcc      52.268344  5042.000406                0.155345         257.464583            2       True        174\n",
      "34       RandomForest_r34_BAG_L2   0.549418         mcc      53.281748  4928.933127                1.168750         144.397304            2       True        169\n",
      "35          LightGBM_r143_BAG_L2   0.549396         mcc      52.464129  4834.524845                0.351130          49.989022            2       True        166\n",
      "36    NeuralNetTorch_r121_BAG_L2   0.549384         mcc      52.285945  5086.532915                0.172947         301.997092            2       True        201\n",
      "37         NeuralNetTorch_BAG_L2   0.549228         mcc      52.290251  4921.088523                0.177252         136.552701            2       True        121\n",
      "38           CatBoost_r50_BAG_L2   0.549064         mcc      52.282456  4807.029791                0.169458          22.493968            2       True        142\n",
      "39           CatBoost_r69_BAG_L2   0.549016         mcc      52.243215  4834.420780                0.130217          49.884957            2       True        146\n",
      "40            XGBoost_r31_BAG_L2   0.548769         mcc      52.276479  4800.266018                0.163481          15.730195            2       True        186\n",
      "41    NeuralNetTorch_r197_BAG_L2   0.548763         mcc      52.266132  4928.850379                0.153134         144.314556            2       True        163\n",
      "42            CatBoost_r5_BAG_L2   0.548748         mcc      52.248725  4839.733940                0.135727          55.198117            2       True        180\n",
      "43           CatBoost_r49_BAG_L2   0.548652         mcc      52.217802  4831.368320                0.104803          46.832497            2       True        164\n",
      "44               CatBoost_BAG_L2   0.548541         mcc      52.220381  4849.289112                0.107382          64.753289            2       True        116\n",
      "45          CatBoost_r137_BAG_L2   0.548212         mcc      52.222243  4838.265583                0.109244          53.729760            2       True        132\n",
      "46           CatBoost_r12_BAG_L2   0.548204         mcc      52.218323  4888.955585                0.105324         104.419762            2       True        205\n",
      "47          CatBoost_r143_BAG_L2   0.548182         mcc      52.227475  4860.020675                0.114477          75.484852            2       True        183\n",
      "48            CatBoost_r9_BAG_L2   0.548124         mcc      52.386672  4815.577610                0.273674          31.041788            2       True        127\n",
      "49     NeuralNetTorch_r30_BAG_L2   0.548085         mcc      52.322940  5063.029737                0.209941         278.493915            2       True        139\n",
      "50           CatBoost_r60_BAG_L2   0.548033         mcc      52.217011  4833.049146                0.104013          48.513323            2       True        189\n",
      "51            CatBoost_r6_BAG_L2   0.547980         mcc      52.254612  4804.303358                0.141614          19.767535            2       True        194\n",
      "52        ExtraTrees_r178_BAG_L2   0.547820         mcc      53.563412  4800.934489                1.450413          16.398666            2       True        184\n",
      "53          CatBoost_r177_BAG_L2   0.547634         mcc      52.218354  4838.227136                0.105356          53.691313            2       True        123\n",
      "54           CatBoost_r86_BAG_L2   0.547593         mcc      52.233500  4952.619236                0.120501         168.083413            2       True        161\n",
      "55           WeightedEnsemble_L2   0.547592         mcc       0.445555   961.961053                0.004759           6.152666            2       True        111\n",
      "56    NeuralNetTorch_r135_BAG_L2   0.547537         mcc      52.311998  4871.703961                0.199000          87.168138            2       True        206\n",
      "57       RandomForest_r15_BAG_L2   0.547359         mcc      53.569637  5014.138991                1.456638         229.603168            2       True        190\n",
      "58           LightGBM_r94_BAG_L2   0.547324         mcc      52.385160  4806.145353                0.272161          21.609530            2       True        170\n",
      "59    NeuralNetTorch_r143_BAG_L1   0.547280         mcc       0.151639   335.787627                0.151639         335.787627            1       True         62\n",
      "60   NeuralNetFastAI_r102_BAG_L2   0.546965         mcc      52.325402  4824.752300                0.212404          40.216477            2       True        133\n",
      "61           LightGBM_r96_BAG_L2   0.546935         mcc      52.294706  4804.476649                0.181707          19.940826            2       True        128\n",
      "62          ExtraTrees_r4_BAG_L2   0.546927         mcc      53.366437  4800.096398                1.253439          15.560575            2       True        175\n",
      "63     NeuralNetTorch_r14_BAG_L1   0.546924         mcc       0.116206   236.676015                0.116206         236.676015            1       True         39\n",
      "64      RandomForest_r127_BAG_L2   0.546809         mcc      53.510515  5077.479908                1.397517         292.944085            2       True        167\n",
      "65         NeuralNetTorch_BAG_L1   0.546769         mcc       0.122773   243.058731                0.122773         243.058731            1       True         12\n",
      "66     NeuralNetTorch_r87_BAG_L2   0.546746         mcc      52.294050  5133.083420                0.181052         348.547597            2       True        181\n",
      "67   NeuralNetFastAI_r138_BAG_L2   0.546591         mcc      52.951504  5155.407009                0.838506         370.871186            2       True        195\n",
      "68     NeuralNetTorch_r71_BAG_L1   0.546245         mcc       0.108680   161.852105                0.108680         161.852105            1       True         73\n",
      "69     NeuralNetTorch_r76_BAG_L2   0.546010         mcc      52.279609  4917.927507                0.166610         133.391684            2       True        199\n",
      "70     NeuralNetTorch_r86_BAG_L1   0.545939         mcc       0.137218   218.774623                0.137218         218.774623            1       True         32\n",
      "71    NeuralNetTorch_r135_BAG_L1   0.545596         mcc       0.151939   401.246137                0.151939         401.246137            1       True         97\n",
      "72      NeuralNetTorch_r1_BAG_L1   0.545353         mcc       0.155859   445.763470                0.155859         445.763470            1       True        109\n",
      "73   NeuralNetFastAI_r191_BAG_L2   0.545291         mcc      52.648121  4989.177153                0.535123         204.641330            2       True        126\n",
      "74    NeuralNetTorch_r185_BAG_L1   0.545052         mcc       0.143829   185.562472                0.143829         185.562472            1       True         78\n",
      "75     NeuralNetTorch_r41_BAG_L1   0.545032         mcc       0.156266   317.722560                0.156266         317.722560            1       True         48\n",
      "76     NeuralNetTorch_r79_BAG_L1   0.544995         mcc       0.145462   221.997735                0.145462         221.997735            1       True         15\n",
      "77     NeuralNetTorch_r22_BAG_L1   0.544835         mcc       0.118884   267.193283                0.118884         267.193283            1       True         20\n",
      "78   NeuralNetFastAI_r134_BAG_L2   0.544769         mcc      52.335862  4836.919930                0.222863          52.384107            2       True        168\n",
      "79        NeuralNetFastAI_BAG_L2   0.544648         mcc      52.633469  4931.382851                0.520471         146.847028            2       True        119\n",
      "80   NeuralNetFastAI_r145_BAG_L2   0.544643         mcc      53.102935  5093.287625                0.989936         308.751802            2       True        137\n",
      "81       RandomForestEntr_BAG_L2   0.544600         mcc      53.707471  4833.073550                1.594472          48.537727            2       True        115\n",
      "82    NeuralNetFastAI_r95_BAG_L2   0.544535         mcc      53.084146  5099.573847                0.971148         315.038024            2       True        156\n",
      "83   NeuralNetFastAI_r172_BAG_L2   0.544483         mcc      52.443878  4868.928994                0.330880          84.393171            2       True        197\n",
      "84       RandomForest_r39_BAG_L2   0.544338         mcc      53.604945  5026.660161                1.491947         242.124338            2       True        154\n",
      "85      RandomForest_r166_BAG_L2   0.544179         mcc      53.647839  4813.168730                1.534840          28.632907            2       True        185\n",
      "86     NeuralNetTorch_r87_BAG_L1   0.544056         mcc       0.143765   376.368381                0.143765         376.368381            1       True         72\n",
      "87      RandomForest_r195_BAG_L2   0.543978         mcc      53.602726  5042.702917                1.489727         258.167094            2       True        135\n",
      "88    NeuralNetTorch_r158_BAG_L2   0.543977         mcc      52.333874  4953.586037                0.220875         169.050214            2       True        160\n",
      "89   NeuralNetFastAI_r127_BAG_L2   0.543877         mcc      52.364068  4835.461632                0.251069          50.925809            2       True        202\n",
      "90        ExtraTrees_r197_BAG_L2   0.543803         mcc      53.680055  4812.264491                1.567056          27.728668            2       True        200\n",
      "91        ExtraTrees_r172_BAG_L2   0.543762         mcc      53.539002  4806.636914                1.426003          22.101091            2       True        145\n",
      "92     NeuralNetTorch_r89_BAG_L1   0.543556         mcc       0.140150   194.831884                0.140150         194.831884            1       True        110\n",
      "93    NeuralNetFastAI_r65_BAG_L2   0.543294         mcc      52.360708  4845.679287                0.247709          61.143465            2       True        176\n",
      "94   NeuralNetFastAI_r194_BAG_L2   0.543201         mcc      52.710888  4905.360896                0.597889         120.825073            2       True        204\n",
      "95         ExtraTrees_r42_BAG_L2   0.543056         mcc      53.652401  4805.539429                1.539403          21.003606            2       True        131\n",
      "96             LightGBMXT_BAG_L1   0.543030         mcc       1.013950    26.571816                1.013950          26.571816            1       True          3\n",
      "97          LightGBM_r188_BAG_L1   0.542969         mcc       1.201027    34.191365                1.201027          34.191365            1       True         27\n",
      "98    NeuralNetTorch_r121_BAG_L1   0.542831         mcc       0.130853   211.995705                0.130853         211.995705            1       True         92\n",
      "99     NeuralNetTorch_r19_BAG_L1   0.542821         mcc       0.113634   121.899851                0.113634         121.899851            1       True        105\n",
      "100    NeuralNetTorch_r76_BAG_L1   0.542681         mcc       0.109447   115.356976                0.109447         115.356976            1       True         90\n",
      "101    NeuralNetTorch_r36_BAG_L1   0.542596         mcc       0.134109   224.900286                0.134109         224.900286            1       True        100\n",
      "102        ExtraTreesEntr_BAG_L2   0.542456         mcc      53.783764  4790.343920                1.670765           5.808098            2       True        118\n",
      "103    NeuralNetTorch_r30_BAG_L1   0.542252         mcc       0.152335   218.894624                0.152335         218.894624            1       True         30\n",
      "104          LightGBM_r42_BAG_L1   0.542063         mcc       1.046410    25.888315                1.046410          25.888315            1       True        108\n",
      "105   NeuralNetFastAI_r69_BAG_L2   0.542050         mcc      52.993448  4995.824524                0.880450         211.288702            2       True        193\n",
      "106   NeuralNetFastAI_r37_BAG_L2   0.541976         mcc      52.493600  4897.957055                0.380602         113.421232            2       True        162\n",
      "107   NeuralNetFastAI_r88_BAG_L2   0.541790         mcc      52.364970  4856.940746                0.251971          72.404923            2       True        177\n",
      "108  NeuralNetFastAI_r103_BAG_L2   0.541652         mcc      52.637288  4975.033291                0.524290         190.497468            2       True        147\n",
      "109   NeuralNetFastAI_r11_BAG_L2   0.541651         mcc      52.990638  5065.827616                0.877640         281.291793            2       True        143\n",
      "110          LightGBM_r96_BAG_L1   0.541578         mcc       3.607537    52.692198                3.607537          52.692198            1       True         19\n",
      "111  NeuralNetFastAI_r156_BAG_L2   0.541458         mcc      52.322712  4830.768327                0.209714          46.232504            2       True        152\n",
      "112   NeuralNetTorch_r197_BAG_L1   0.541233         mcc       0.103559    89.198776                0.103559          89.198776            1       True         54\n",
      "113    NeuralNetTorch_r31_BAG_L1   0.541203         mcc       0.126527   159.185462                0.126527         159.185462            1       True         65\n",
      "114  NeuralNetFastAI_r143_BAG_L2   0.541137         mcc      52.360213  4856.177664                0.247214          71.641841            2       True        150\n",
      "115  NeuralNetFastAI_r160_BAG_L2   0.540993         mcc      53.152404  4990.398985                1.039406         205.863162            2       True        188\n",
      "116      RandomForestGini_BAG_L2   0.540734         mcc      53.735013  4823.298352                1.622015          38.762529            2       True        114\n",
      "117  NeuralNetFastAI_r187_BAG_L1   0.540687         mcc       0.191251    71.380031                0.191251          71.380031            1       True        104\n",
      "118      RandomForest_r16_BAG_L2   0.540452         mcc      53.626751  5135.869805                1.513752         351.333982            2       True        203\n",
      "119         LightGBM_r196_BAG_L1   0.540413         mcc      13.361557   132.678803               13.361557         132.678803            1       True         44\n",
      "120           XGBoost_r89_BAG_L1   0.540267         mcc       0.057261    15.729465                0.057261          15.729465            1       True         29\n",
      "121  NeuralNetFastAI_r111_BAG_L2   0.540180         mcc      52.315101  4820.103053                0.202103          35.567230            2       True        173\n",
      "122  NeuralNetFastAI_r156_BAG_L1   0.540078         mcc       0.144121    44.023052                0.144121          44.023052            1       True         43\n",
      "123           XGBoost_r49_BAG_L1   0.540016         mcc       0.066474    17.446805                0.066474          17.446805            1       True         70\n",
      "124  NeuralNetFastAI_r145_BAG_L1   0.539930         mcc       0.949654   295.569705                0.949654         295.569705            1       True         28\n",
      "125               XGBoost_BAG_L1   0.539687         mcc       0.050316    12.490709                0.050316          12.490709            1       True         11\n",
      "126          LightGBM_r94_BAG_L1   0.538971         mcc       1.929225    40.568026                1.929225          40.568026            1       True         61\n",
      "127          XGBoost_r194_BAG_L1   0.538964         mcc       0.046736    12.471031                0.046736          12.471031            1       True         35\n",
      "128  NeuralNetFastAI_r191_BAG_L1   0.538927         mcc       0.459656   199.218875                0.459656         199.218875            1       True         17\n",
      "129  NeuralNetFastAI_r134_BAG_L1   0.538913         mcc       0.144029    48.603300                0.144029          48.603300            1       True         59\n",
      "130          LightGBM_r30_BAG_L1   0.538795         mcc       3.689649    57.596646                3.689649          57.596646            1       True         69\n",
      "131        ExtraTrees_r49_BAG_L2   0.538759         mcc      53.860756  4789.721097                1.747758           5.185274            2       True        165\n",
      "132        ExtraTreesGini_BAG_L2   0.538759         mcc      53.887063  4789.810085                1.774065           5.274262            2       True        117\n",
      "133  NeuralNetFastAI_r194_BAG_L1   0.538692         mcc       0.447586   116.233874                0.447586         116.233874            1       True         95\n",
      "134  NeuralNetFastAI_r100_BAG_L1   0.538630         mcc       0.154325    43.363926                0.154325          43.363926            1       True        101\n",
      "135           XGBoost_r22_BAG_L1   0.538550         mcc       0.051449    13.088616                0.051449          13.088616            1       True         83\n",
      "136   NeuralNetTorch_r158_BAG_L1   0.538419         mcc       0.156125   253.766608                0.156125         253.766608            1       True         51\n",
      "137          CatBoost_r70_BAG_L1   0.538413         mcc       0.183660    24.470315                0.183660          24.470315            1       True         42\n",
      "138       NeuralNetFastAI_BAG_L1   0.538315         mcc       0.498106   146.628534                0.498106         146.628534            1       True         10\n",
      "139  NeuralNetFastAI_r138_BAG_L1   0.538291         mcc       0.861640   384.066492                0.861640         384.066492            1       True         86\n",
      "140       ExtraTrees_r178_BAG_L1   0.538237         mcc       1.134003     3.352153                1.134003           3.352153            1       True         75\n",
      "141           CatBoost_r6_BAG_L1   0.538087         mcc       0.168535    22.118754                0.168535          22.118754            1       True         85\n",
      "142          CatBoost_r50_BAG_L1   0.537860         mcc       0.176726    24.638916                0.176726          24.638916            1       True         33\n",
      "143  NeuralNetFastAI_r102_BAG_L1   0.537781         mcc       0.149170    37.271152                0.149170          37.271152            1       True         24\n",
      "144         LightGBM_r130_BAG_L1   0.537780         mcc       0.380405    22.918816                0.380405          22.918816            1       True         31\n",
      "145          CatBoost_r86_BAG_L1   0.537638         mcc       0.043207   217.430173                0.043207         217.430173            1       True         52\n",
      "146           XGBoost_r33_BAG_L1   0.537562         mcc       0.090855    18.686382                0.090855          18.686382            1       True         21\n",
      "147         LightGBMLarge_BAG_L1   0.537542         mcc       0.358828    27.112571                0.358828          27.112571            1       True         13\n",
      "148              LightGBM_BAG_L1   0.537474         mcc       0.410336    19.187984                0.410336          19.187984            1       True          4\n",
      "149          CatBoost_r13_BAG_L1   0.537303         mcc       0.054496   370.716664                0.054496         370.716664            1       True         25\n",
      "150           XGBoost_r95_BAG_L1   0.537278         mcc       0.056599    15.195037                0.056599          15.195037            1       True        106\n",
      "151  NeuralNetFastAI_r127_BAG_L1   0.537228         mcc       0.185067    48.331262                0.185067          48.331262            1       True         93\n",
      "152   NeuralNetFastAI_r95_BAG_L1   0.537151         mcc       0.839535   304.552556                0.839535         304.552556            1       True         47\n",
      "153       ExtraTrees_r172_BAG_L1   0.537086         mcc       1.221390     4.213252                1.221390           4.213252            1       True         36\n",
      "154           CatBoost_r9_BAG_L1   0.536837         mcc       0.181886    29.727680                0.181886          29.727680            1       True         18\n",
      "155   NeuralNetFastAI_r65_BAG_L1   0.536820         mcc       0.177656    58.116469                0.177656          58.116469            1       True         67\n",
      "156         LightGBM_r131_BAG_L1   0.536643         mcc       0.941054    29.797539                0.941054          29.797539            1       True         16\n",
      "157   NeuralNetFastAI_r37_BAG_L1   0.536614         mcc       0.278218   113.325541                0.278218         113.325541            1       True         53\n",
      "158         ExtraTrees_r4_BAG_L1   0.536583         mcc       0.955531     3.078708                0.955531           3.078708            1       True         66\n",
      "159         LightGBM_r121_BAG_L1   0.536440         mcc       0.974863    45.936775                0.974863          45.936775            1       True         87\n",
      "160         LightGBM_r143_BAG_L1   0.536317         mcc       0.600308    35.182001                0.600308          35.182001            1       True         57\n",
      "161  NeuralNetFastAI_r172_BAG_L1   0.536301         mcc       0.261135    80.325711                0.261135          80.325711            1       True         88\n",
      "162         CatBoost_r198_BAG_L1   0.536220         mcc       0.044554   108.738023                0.044554         108.738023            1       True        103\n",
      "163          CatBoost_r12_BAG_L1   0.536054         mcc       0.039603   155.997320                0.039603         155.997320            1       True         96\n",
      "164           XGBoost_r34_BAG_L1   0.535991         mcc       0.074235    17.021468                0.074235          17.021468            1       True        107\n",
      "165          LightGBM_r15_BAG_L1   0.535950         mcc       0.694780    25.532577                0.694780          25.532577            1       True         50\n",
      "166              CatBoost_BAG_L1   0.535749         mcc       0.035131    97.167740                0.035131          97.167740            1       True          7\n",
      "167         CatBoost_r167_BAG_L1   0.535488         mcc       0.028368    84.907089                0.028368          84.907089            1       True         46\n",
      "168   NeuralNetFastAI_r69_BAG_L1   0.535410         mcc       0.811603   209.074501                0.811603         209.074501            1       True         84\n",
      "169         LightGBM_r161_BAG_L1   0.535185         mcc       0.901815    45.786468                0.901815          45.786468            1       True         40\n",
      "170  NeuralNetFastAI_r103_BAG_L1   0.535185         mcc       0.431829   185.587901                0.431829         185.587901            1       True         38\n",
      "171          CatBoost_r69_BAG_L1   0.535092         mcc       0.033308    75.112781                0.033308          75.112781            1       True         37\n",
      "172         LightGBM_r135_BAG_L1   0.534971         mcc       0.221590    25.457825                0.221590          25.457825            1       True         82\n",
      "173         CatBoost_r163_BAG_L1   0.534942         mcc       0.040316    58.855360                0.040316          58.855360            1       True        102\n",
      "174         CatBoost_r180_BAG_L1   0.534603         mcc       0.106939    22.485612                0.106939          22.485612            1       True         89\n",
      "175          CatBoost_r60_BAG_L1   0.534584         mcc       0.040659    81.149150                0.040659          81.149150            1       True         80\n",
      "176         CatBoost_r177_BAG_L1   0.534531         mcc       0.028861    69.270645                0.028861          69.270645            1       True         14\n",
      "177         CatBoost_r143_BAG_L1   0.534531         mcc       0.030644    79.997368                0.030644          79.997368            1       True         74\n",
      "178  NeuralNetFastAI_r111_BAG_L1   0.534512         mcc       0.137868    32.544040                0.137868          32.544040            1       True         64\n",
      "179          CatBoost_r49_BAG_L1   0.534481         mcc       0.039640    78.280669                0.039640          78.280669            1       True         55\n",
      "180         CatBoost_r137_BAG_L1   0.534331         mcc       0.041925    87.099009                0.041925          87.099009            1       True         23\n",
      "181       ExtraTrees_r126_BAG_L1   0.534263         mcc       1.076496     2.219787                1.076496           2.219787            1       True         99\n",
      "182   NeuralNetFastAI_r88_BAG_L1   0.533908         mcc       0.187677    57.306049                0.187677          57.306049            1       True         68\n",
      "183           CatBoost_r5_BAG_L1   0.533379         mcc       0.038219    78.872919                0.038219          78.872919            1       True         71\n",
      "184   NeuralNetFastAI_r11_BAG_L1   0.533341         mcc       0.769256   269.878860                0.769256         269.878860            1       True         34\n",
      "185         CatBoost_r128_BAG_L1   0.533096         mcc       0.137632    26.017408                0.137632          26.017408            1       True         63\n",
      "186           XGBoost_r98_BAG_L1   0.532662         mcc       0.093484    24.627177                0.093484          24.627177            1       True         49\n",
      "187  NeuralNetFastAI_r160_BAG_L1   0.532660         mcc       0.838983   200.724497                0.838983         200.724497            1       True         79\n",
      "188      RandomForest_r34_BAG_L1   0.531571         mcc       0.907082    10.178008                0.907082          10.178008            1       True         60\n",
      "189  NeuralNetFastAI_r143_BAG_L1   0.531152         mcc       0.201651    66.001769                0.201651          66.001769            1       True         41\n",
      "190    NeuralNetFastAI_r4_BAG_L1   0.530165         mcc       0.436032   148.053186                0.436032         148.053186            1       True         98\n",
      "191        ExtraTreesEntr_BAG_L1   0.521519         mcc       1.581409     3.531258                1.581409           3.531258            1       True          9\n",
      "192        ExtraTrees_r49_BAG_L1   0.521382         mcc       1.546747     3.698784                1.546747           3.698784            1       True         56\n",
      "193        ExtraTreesGini_BAG_L1   0.521382         mcc       1.547668     3.641959                1.547668           3.641959            1       True          8\n",
      "194           XGBoost_r31_BAG_L1   0.520802         mcc       0.084693    24.135002                0.084693          24.135002            1       True         77\n",
      "195     RandomForest_r127_BAG_L1   0.518765         mcc       1.104316    16.952986                1.104316          16.952986            1       True         58\n",
      "196      RandomForestGini_BAG_L1   0.517651         mcc       1.316420     7.330471                1.316420           7.330471            1       True          5\n",
      "197     RandomForest_r166_BAG_L1   0.517651         mcc       1.333955     7.226185                1.333955           7.226185            1       True         76\n",
      "198      RandomForest_r15_BAG_L1   0.516976         mcc       1.171182    14.133073                1.171182          14.133073            1       True         81\n",
      "199      RandomForestEntr_BAG_L1   0.516385         mcc       1.326219     8.452163                1.326219           8.452163            1       True          6\n",
      "200      RandomForest_r39_BAG_L1   0.514666         mcc       1.203424    13.905938                1.203424          13.905938            1       True         45\n",
      "201        ExtraTrees_r42_BAG_L1   0.514083         mcc       1.428896     4.798207                1.428896           4.798207            1       True         22\n",
      "202       ExtraTrees_r197_BAG_L1   0.511137         mcc       1.425242     5.765684                1.425242           5.765684            1       True         91\n",
      "203     RandomForest_r195_BAG_L1   0.510650         mcc       1.244525    14.681567                1.244525          14.681567            1       True         26\n",
      "204      RandomForest_r16_BAG_L1   0.504119         mcc       1.279437    19.261120                1.279437          19.261120            1       True         94\n",
      "205        KNeighborsUnif_BAG_L1   0.430447         mcc       0.158188     0.271386                0.158188           0.271386            1       True          1\n",
      "206        KNeighborsDist_BAG_L1   0.424524         mcc       0.162324     0.054117                0.162324           0.054117            1       True          2\n",
      "Number of models trained: 207\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_XT', 'WeightedEnsembleModel', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_TabularNeuralNetTorch', 'StackerEnsembleModel_XGBoost'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 4 | ['소비 빈도 및 활동성', '소비 금액 및 규모', '연체 가능성 및 신용 리스크', '서비스 이용 다양성']\n",
      "('int', ['bool']) : 8 | ['남성', '여성', '20대', '30대', '40대', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor(label='연체여부',\n",
    "                            eval_metric='mcc',\n",
    "                            problem_type='binary').fit(auto_df_train,\n",
    "                                                       presets='best_quality',\n",
    "                                                        time_limit=3600*8,\n",
    "                                                       raise_on_no_models_fitted=False,\n",
    "                                                    #    excluded_model_types=['KNN','XGBoost'],\n",
    "                                                       ag_args_fit={'num_gpus': 1},\n",
    "                                                       auto_stack=True,\n",
    "#                                                        num_bag_folds=5,\n",
    "#                                                        num_stack_levels=2,\n",
    "#                                                        num_bag_sets = 1,\n",
    "                                                      )\n",
    "results = predictor.fit_summary(verbosity=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87cb0048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T14:02:21.797950Z",
     "iopub.status.busy": "2025-01-06T14:02:21.797335Z",
     "iopub.status.idle": "2025-01-06T15:50:53.931059Z",
     "shell.execute_reply": "2025-01-06T15:50:53.929901Z"
    },
    "papermill": {
     "duration": 6512.293235,
     "end_time": "2025-01-06T15:50:54.019529",
     "exception": false,
     "start_time": "2025-01-06T14:02:21.726294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': -0.04860246181385666,\n",
       " 'accuracy': 0.714485,\n",
       " 'balanced_accuracy': 0.48906528068901445,\n",
       " 'roc_auc': 0.45328720091797225,\n",
       " 'f1': 0.04169230597106768,\n",
       " 'precision': 0.15393353574157836,\n",
       " 'recall': 0.02411139184813026}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 예측(라벨값 제외)\n",
    "y_pred = predictor.predict(auto_df_test)\n",
    "\n",
    "## 모델 평가\n",
    "evaluation_results = predictor.evaluate(auto_df_test, silent=True)\n",
    "evaluation_results"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6431154,
     "sourceId": 10381668,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35347.646742,
   "end_time": "2025-01-06T15:50:56.946852",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-06T06:01:49.300110",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
